{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge, Unit 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "auc_scoring = make_scorer(roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('cervical_cancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Number of sexual partners</th>\n",
       "      <th>First sexual intercourse</th>\n",
       "      <th>Num of pregnancies</th>\n",
       "      <th>Smokes</th>\n",
       "      <th>Smokes (years)</th>\n",
       "      <th>Smokes (packs/year)</th>\n",
       "      <th>Hormonal Contraceptives</th>\n",
       "      <th>Hormonal Contraceptives (years)</th>\n",
       "      <th>IUD</th>\n",
       "      <th>...</th>\n",
       "      <th>STDs: Time since first diagnosis</th>\n",
       "      <th>STDs: Time since last diagnosis</th>\n",
       "      <th>Dx:Cancer</th>\n",
       "      <th>Dx:CIN</th>\n",
       "      <th>Dx:HPV</th>\n",
       "      <th>Dx</th>\n",
       "      <th>Hinselmann</th>\n",
       "      <th>Schiller</th>\n",
       "      <th>Citology</th>\n",
       "      <th>Biopsy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Number of sexual partners First sexual intercourse Num of pregnancies  \\\n",
       "0   18                       4.0                     15.0                1.0   \n",
       "1   15                       1.0                     14.0                1.0   \n",
       "2   34                       1.0                        ?                1.0   \n",
       "3   52                       5.0                     16.0                4.0   \n",
       "4   46                       3.0                     21.0                4.0   \n",
       "\n",
       "  Smokes Smokes (years) Smokes (packs/year) Hormonal Contraceptives  \\\n",
       "0    0.0            0.0                 0.0                     0.0   \n",
       "1    0.0            0.0                 0.0                     0.0   \n",
       "2    0.0            0.0                 0.0                     0.0   \n",
       "3    1.0           37.0                37.0                     1.0   \n",
       "4    0.0            0.0                 0.0                     1.0   \n",
       "\n",
       "  Hormonal Contraceptives (years)  IUD  ... STDs: Time since first diagnosis  \\\n",
       "0                             0.0  0.0  ...                                ?   \n",
       "1                             0.0  0.0  ...                                ?   \n",
       "2                             0.0  0.0  ...                                ?   \n",
       "3                             3.0  0.0  ...                                ?   \n",
       "4                            15.0  0.0  ...                                ?   \n",
       "\n",
       "  STDs: Time since last diagnosis Dx:Cancer Dx:CIN Dx:HPV Dx Hinselmann  \\\n",
       "0                               ?         0      0      0  0          0   \n",
       "1                               ?         0      0      0  0          0   \n",
       "2                               ?         0      0      0  0          0   \n",
       "3                               ?         1      0      1  0          0   \n",
       "4                               ?         0      0      0  0          0   \n",
       "\n",
       "  Schiller Citology Biopsy  \n",
       "0        0        0      0  \n",
       "1        0        0      0  \n",
       "2        0        0      0  \n",
       "3        0        0      0  \n",
       "4        0        0      0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 858 entries, 0 to 857\n",
      "Data columns (total 36 columns):\n",
      "Age                                   858 non-null int64\n",
      "Number of sexual partners             858 non-null object\n",
      "First sexual intercourse              858 non-null object\n",
      "Num of pregnancies                    858 non-null object\n",
      "Smokes                                858 non-null object\n",
      "Smokes (years)                        858 non-null object\n",
      "Smokes (packs/year)                   858 non-null object\n",
      "Hormonal Contraceptives               858 non-null object\n",
      "Hormonal Contraceptives (years)       858 non-null object\n",
      "IUD                                   858 non-null object\n",
      "IUD (years)                           858 non-null object\n",
      "STDs                                  858 non-null object\n",
      "STDs (number)                         858 non-null object\n",
      "STDs:condylomatosis                   858 non-null object\n",
      "STDs:cervical condylomatosis          858 non-null object\n",
      "STDs:vaginal condylomatosis           858 non-null object\n",
      "STDs:vulvo-perineal condylomatosis    858 non-null object\n",
      "STDs:syphilis                         858 non-null object\n",
      "STDs:pelvic inflammatory disease      858 non-null object\n",
      "STDs:genital herpes                   858 non-null object\n",
      "STDs:molluscum contagiosum            858 non-null object\n",
      "STDs:AIDS                             858 non-null object\n",
      "STDs:HIV                              858 non-null object\n",
      "STDs:Hepatitis B                      858 non-null object\n",
      "STDs:HPV                              858 non-null object\n",
      "STDs: Number of diagnosis             858 non-null int64\n",
      "STDs: Time since first diagnosis      858 non-null object\n",
      "STDs: Time since last diagnosis       858 non-null object\n",
      "Dx:Cancer                             858 non-null int64\n",
      "Dx:CIN                                858 non-null int64\n",
      "Dx:HPV                                858 non-null int64\n",
      "Dx                                    858 non-null int64\n",
      "Hinselmann                            858 non-null int64\n",
      "Schiller                              858 non-null int64\n",
      "Citology                              858 non-null int64\n",
      "Biopsy                                858 non-null int64\n",
      "dtypes: int64(10), object(26)\n",
      "memory usage: 241.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>STDs: Number of diagnosis</th>\n",
       "      <th>Dx:Cancer</th>\n",
       "      <th>Dx:CIN</th>\n",
       "      <th>Dx:HPV</th>\n",
       "      <th>Dx</th>\n",
       "      <th>Hinselmann</th>\n",
       "      <th>Schiller</th>\n",
       "      <th>Citology</th>\n",
       "      <th>Biopsy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>858.000000</td>\n",
       "      <td>858.000000</td>\n",
       "      <td>858.000000</td>\n",
       "      <td>858.000000</td>\n",
       "      <td>858.000000</td>\n",
       "      <td>858.000000</td>\n",
       "      <td>858.000000</td>\n",
       "      <td>858.000000</td>\n",
       "      <td>858.000000</td>\n",
       "      <td>858.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>26.820513</td>\n",
       "      <td>0.087413</td>\n",
       "      <td>0.020979</td>\n",
       "      <td>0.010490</td>\n",
       "      <td>0.020979</td>\n",
       "      <td>0.027972</td>\n",
       "      <td>0.040793</td>\n",
       "      <td>0.086247</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.064103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>8.497948</td>\n",
       "      <td>0.302545</td>\n",
       "      <td>0.143398</td>\n",
       "      <td>0.101939</td>\n",
       "      <td>0.143398</td>\n",
       "      <td>0.164989</td>\n",
       "      <td>0.197925</td>\n",
       "      <td>0.280892</td>\n",
       "      <td>0.220701</td>\n",
       "      <td>0.245078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age  STDs: Number of diagnosis   Dx:Cancer      Dx:CIN  \\\n",
       "count  858.000000                 858.000000  858.000000  858.000000   \n",
       "mean    26.820513                   0.087413    0.020979    0.010490   \n",
       "std      8.497948                   0.302545    0.143398    0.101939   \n",
       "min     13.000000                   0.000000    0.000000    0.000000   \n",
       "25%     20.000000                   0.000000    0.000000    0.000000   \n",
       "50%     25.000000                   0.000000    0.000000    0.000000   \n",
       "75%     32.000000                   0.000000    0.000000    0.000000   \n",
       "max     84.000000                   3.000000    1.000000    1.000000   \n",
       "\n",
       "           Dx:HPV          Dx  Hinselmann    Schiller    Citology      Biopsy  \n",
       "count  858.000000  858.000000  858.000000  858.000000  858.000000  858.000000  \n",
       "mean     0.020979    0.027972    0.040793    0.086247    0.051282    0.064103  \n",
       "std      0.143398    0.164989    0.197925    0.280892    0.220701    0.245078  \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                                   0.0\n",
       "Number of sexual partners             0.0\n",
       "First sexual intercourse              0.0\n",
       "Num of pregnancies                    0.0\n",
       "Smokes                                0.0\n",
       "Smokes (years)                        0.0\n",
       "Smokes (packs/year)                   0.0\n",
       "Hormonal Contraceptives               0.0\n",
       "Hormonal Contraceptives (years)       0.0\n",
       "IUD                                   0.0\n",
       "IUD (years)                           0.0\n",
       "STDs                                  0.0\n",
       "STDs (number)                         0.0\n",
       "STDs:condylomatosis                   0.0\n",
       "STDs:cervical condylomatosis          0.0\n",
       "STDs:vaginal condylomatosis           0.0\n",
       "STDs:vulvo-perineal condylomatosis    0.0\n",
       "STDs:syphilis                         0.0\n",
       "STDs:pelvic inflammatory disease      0.0\n",
       "STDs:genital herpes                   0.0\n",
       "STDs:molluscum contagiosum            0.0\n",
       "STDs:AIDS                             0.0\n",
       "STDs:HIV                              0.0\n",
       "STDs:Hepatitis B                      0.0\n",
       "STDs:HPV                              0.0\n",
       "STDs: Number of diagnosis             0.0\n",
       "STDs: Time since first diagnosis      0.0\n",
       "STDs: Time since last diagnosis       0.0\n",
       "Dx:Cancer                             0.0\n",
       "Dx:CIN                                0.0\n",
       "Dx:HPV                                0.0\n",
       "Dx                                    0.0\n",
       "Hinselmann                            0.0\n",
       "Schiller                              0.0\n",
       "Citology                              0.0\n",
       "Biopsy                                0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()*100/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age 0\n",
      "STDs: Number of diagnosis 0\n",
      "Dx:Cancer 0\n",
      "Dx:CIN 0\n",
      "Dx:HPV 0\n",
      "Dx 0\n",
      "Hinselmann 0\n",
      "Schiller 0\n",
      "Citology 0\n",
      "Biopsy 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].dtype == int:\n",
    "         print(col,df[col][df[col] == '?'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sexual partners 26\n",
      "First sexual intercourse 7\n",
      "Num of pregnancies 56\n",
      "Smokes 13\n",
      "Smokes (years) 13\n",
      "Smokes (packs/year) 13\n",
      "Hormonal Contraceptives 108\n",
      "Hormonal Contraceptives (years) 108\n",
      "IUD 117\n",
      "IUD (years) 117\n",
      "STDs 105\n",
      "STDs (number) 105\n",
      "STDs:condylomatosis 105\n",
      "STDs:cervical condylomatosis 105\n",
      "STDs:vaginal condylomatosis 105\n",
      "STDs:vulvo-perineal condylomatosis 105\n",
      "STDs:syphilis 105\n",
      "STDs:pelvic inflammatory disease 105\n",
      "STDs:genital herpes 105\n",
      "STDs:molluscum contagiosum 105\n",
      "STDs:AIDS 105\n",
      "STDs:HIV 105\n",
      "STDs:Hepatitis B 105\n",
      "STDs:HPV 105\n",
      "STDs: Time since first diagnosis 787\n",
      "STDs: Time since last diagnosis 787\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "         print(col,df[col][df[col] == '?'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('?', np.nan)\n",
    "df= df.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                                     0\n",
       "Number of sexual partners              26\n",
       "First sexual intercourse                7\n",
       "Num of pregnancies                     56\n",
       "Smokes                                 13\n",
       "Smokes (years)                         13\n",
       "Smokes (packs/year)                    13\n",
       "Hormonal Contraceptives               108\n",
       "Hormonal Contraceptives (years)       108\n",
       "IUD                                   117\n",
       "IUD (years)                           117\n",
       "STDs                                  105\n",
       "STDs (number)                         105\n",
       "STDs:condylomatosis                   105\n",
       "STDs:cervical condylomatosis          105\n",
       "STDs:vaginal condylomatosis           105\n",
       "STDs:vulvo-perineal condylomatosis    105\n",
       "STDs:syphilis                         105\n",
       "STDs:pelvic inflammatory disease      105\n",
       "STDs:genital herpes                   105\n",
       "STDs:molluscum contagiosum            105\n",
       "STDs:AIDS                             105\n",
       "STDs:HIV                              105\n",
       "STDs:Hepatitis B                      105\n",
       "STDs:HPV                              105\n",
       "STDs: Number of diagnosis               0\n",
       "STDs: Time since first diagnosis      787\n",
       "STDs: Time since last diagnosis       787\n",
       "Dx:Cancer                               0\n",
       "Dx:CIN                                  0\n",
       "Dx:HPV                                  0\n",
       "Dx                                      0\n",
       "Hinselmann                              0\n",
       "Schiller                                0\n",
       "Citology                                0\n",
       "Biopsy                                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#too many '?' values\n",
    "df.drop('STDs: Time since first diagnosis', axis=1, inplace=True)\n",
    "df.drop('STDs: Time since last diagnosis', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    803\n",
       "1     55\n",
       "Name: Biopsy, dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Biopsy.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Number of sexual partners'] = df['Number of sexual partners'].fillna(df['Number of sexual partners'].median())\n",
    "df['First sexual intercourse'] = df['First sexual intercourse'].fillna(df['First sexual intercourse'].median())\n",
    "df['Num of pregnancies'] = df['Num of pregnancies'].fillna(df['Num of pregnancies'].median())\n",
    "df['Smokes'] = df['Smokes'].fillna(1)\n",
    "df['Smokes (years)'] = df['Smokes (years)'].fillna(df['Smokes (years)'].median())\n",
    "df['Smokes (packs/year)'] = df['Smokes (packs/year)'].fillna(df['Smokes (packs/year)'].median())\n",
    "df['Hormonal Contraceptives'] = df['Hormonal Contraceptives'].fillna(1)\n",
    "df['Hormonal Contraceptives (years)'] = df['Hormonal Contraceptives (years)'].fillna(df['Hormonal Contraceptives (years)'].median())\n",
    "df['IUD'] = df['IUD'].fillna(0) # Under suggestion\n",
    "df['IUD (years)'] = df['IUD (years)'].fillna(0) #Under suggestion\n",
    "df['STDs'] = df['STDs'].fillna(1)\n",
    "df['STDs (number)'] = df['STDs (number)'].fillna(df['STDs (number)'].median())\n",
    "df['STDs:condylomatosis'] = df['STDs:condylomatosis'].fillna(df['STDs:condylomatosis'].median())\n",
    "df['STDs:cervical condylomatosis'] = df['STDs:cervical condylomatosis'].fillna(df['STDs:cervical condylomatosis'].median())\n",
    "df['STDs:vaginal condylomatosis'] = df['STDs:vaginal condylomatosis'].fillna(df['STDs:vaginal condylomatosis'].median())\n",
    "df['STDs:vulvo-perineal condylomatosis'] = df['STDs:vulvo-perineal condylomatosis'].fillna(df['STDs:vulvo-perineal condylomatosis'].median())\n",
    "df['STDs:syphilis'] = df['STDs:syphilis'].fillna(df['STDs:syphilis'].median())\n",
    "df['STDs:pelvic inflammatory disease'] = df['STDs:pelvic inflammatory disease'].fillna(df['STDs:pelvic inflammatory disease'].median())\n",
    "df['STDs:genital herpes'] = df['STDs:genital herpes'].fillna(df['STDs:genital herpes'].median())\n",
    "df['STDs:molluscum contagiosum'] = df['STDs:molluscum contagiosum'].fillna(df['STDs:molluscum contagiosum'].median())\n",
    "df['STDs:AIDS'] = df['STDs:AIDS'].fillna(df['STDs:AIDS'].median())\n",
    "df['STDs:HIV'] = df['STDs:HIV'].fillna(df['STDs:HIV'].median())\n",
    "df['STDs:Hepatitis B'] = df['STDs:Hepatitis B'].fillna(df['STDs:Hepatitis B'].median())\n",
    "df['STDs:HPV'] = df['STDs:HPV'].fillna(df['STDs:HPV'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Number of sexual partners</th>\n",
       "      <th>First sexual intercourse</th>\n",
       "      <th>Num of pregnancies</th>\n",
       "      <th>Smokes</th>\n",
       "      <th>Smokes (years)</th>\n",
       "      <th>Smokes (packs/year)</th>\n",
       "      <th>Hormonal Contraceptives</th>\n",
       "      <th>Hormonal Contraceptives (years)</th>\n",
       "      <th>IUD</th>\n",
       "      <th>...</th>\n",
       "      <th>STDs:HPV</th>\n",
       "      <th>STDs: Number of diagnosis</th>\n",
       "      <th>Dx:Cancer</th>\n",
       "      <th>Dx:CIN</th>\n",
       "      <th>Dx:HPV</th>\n",
       "      <th>Dx</th>\n",
       "      <th>Hinselmann</th>\n",
       "      <th>Schiller</th>\n",
       "      <th>Citology</th>\n",
       "      <th>Biopsy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Age</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.085971</td>\n",
       "      <td>0.369175</td>\n",
       "      <td>0.525892</td>\n",
       "      <td>0.045244</td>\n",
       "      <td>0.218261</td>\n",
       "      <td>0.131861</td>\n",
       "      <td>0.029201</td>\n",
       "      <td>0.295267</td>\n",
       "      <td>0.279429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040861</td>\n",
       "      <td>-0.001606</td>\n",
       "      <td>0.110340</td>\n",
       "      <td>0.061443</td>\n",
       "      <td>0.101722</td>\n",
       "      <td>0.092635</td>\n",
       "      <td>-0.003967</td>\n",
       "      <td>0.103283</td>\n",
       "      <td>-0.016862</td>\n",
       "      <td>0.055956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Number of sexual partners</td>\n",
       "      <td>0.085971</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.145847</td>\n",
       "      <td>0.077439</td>\n",
       "      <td>0.243479</td>\n",
       "      <td>0.175729</td>\n",
       "      <td>0.174968</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.021188</td>\n",
       "      <td>0.032460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014360</td>\n",
       "      <td>0.053056</td>\n",
       "      <td>0.023699</td>\n",
       "      <td>0.016669</td>\n",
       "      <td>0.028646</td>\n",
       "      <td>0.024597</td>\n",
       "      <td>-0.039098</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>0.024067</td>\n",
       "      <td>-0.000408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>First sexual intercourse</td>\n",
       "      <td>0.369175</td>\n",
       "      <td>-0.145847</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.056374</td>\n",
       "      <td>-0.119365</td>\n",
       "      <td>-0.058207</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>-0.009232</td>\n",
       "      <td>0.025071</td>\n",
       "      <td>-0.010758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034728</td>\n",
       "      <td>-0.013331</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>-0.032628</td>\n",
       "      <td>0.043964</td>\n",
       "      <td>0.035748</td>\n",
       "      <td>-0.016549</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>-0.010974</td>\n",
       "      <td>0.007259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Num of pregnancies</td>\n",
       "      <td>0.525892</td>\n",
       "      <td>0.077439</td>\n",
       "      <td>-0.056374</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068307</td>\n",
       "      <td>0.175832</td>\n",
       "      <td>0.096976</td>\n",
       "      <td>0.118938</td>\n",
       "      <td>0.218805</td>\n",
       "      <td>0.204501</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026153</td>\n",
       "      <td>0.034912</td>\n",
       "      <td>0.036962</td>\n",
       "      <td>-0.002600</td>\n",
       "      <td>0.048578</td>\n",
       "      <td>0.014227</td>\n",
       "      <td>0.037809</td>\n",
       "      <td>0.085810</td>\n",
       "      <td>-0.027675</td>\n",
       "      <td>0.040215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Smokes</td>\n",
       "      <td>0.045244</td>\n",
       "      <td>0.243479</td>\n",
       "      <td>-0.119365</td>\n",
       "      <td>0.068307</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.682009</td>\n",
       "      <td>0.465475</td>\n",
       "      <td>-0.002485</td>\n",
       "      <td>0.028679</td>\n",
       "      <td>-0.044870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045197</td>\n",
       "      <td>0.117277</td>\n",
       "      <td>0.003270</td>\n",
       "      <td>-0.044686</td>\n",
       "      <td>0.025538</td>\n",
       "      <td>-0.054271</td>\n",
       "      <td>0.039562</td>\n",
       "      <td>0.059913</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.029733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Smokes (years)</td>\n",
       "      <td>0.218261</td>\n",
       "      <td>0.175729</td>\n",
       "      <td>-0.058207</td>\n",
       "      <td>0.175832</td>\n",
       "      <td>0.682009</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724320</td>\n",
       "      <td>-0.013888</td>\n",
       "      <td>0.052006</td>\n",
       "      <td>0.027492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051201</td>\n",
       "      <td>0.078303</td>\n",
       "      <td>0.052859</td>\n",
       "      <td>-0.030476</td>\n",
       "      <td>0.055398</td>\n",
       "      <td>-0.050213</td>\n",
       "      <td>0.070352</td>\n",
       "      <td>0.093479</td>\n",
       "      <td>-0.007275</td>\n",
       "      <td>0.061204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Smokes (packs/year)</td>\n",
       "      <td>0.131861</td>\n",
       "      <td>0.174968</td>\n",
       "      <td>-0.056232</td>\n",
       "      <td>0.096976</td>\n",
       "      <td>0.465475</td>\n",
       "      <td>0.724320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.043262</td>\n",
       "      <td>0.008226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008015</td>\n",
       "      <td>0.029912</td>\n",
       "      <td>0.107229</td>\n",
       "      <td>-0.020800</td>\n",
       "      <td>0.109118</td>\n",
       "      <td>-0.034270</td>\n",
       "      <td>0.026086</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.004250</td>\n",
       "      <td>0.024487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Hormonal Contraceptives</td>\n",
       "      <td>0.029201</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>-0.009232</td>\n",
       "      <td>0.118938</td>\n",
       "      <td>-0.002485</td>\n",
       "      <td>-0.013888</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.385833</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032666</td>\n",
       "      <td>-0.062199</td>\n",
       "      <td>0.011278</td>\n",
       "      <td>-0.004397</td>\n",
       "      <td>0.028808</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>0.012360</td>\n",
       "      <td>-0.034002</td>\n",
       "      <td>-0.025116</td>\n",
       "      <td>-0.018015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Hormonal Contraceptives (years)</td>\n",
       "      <td>0.295267</td>\n",
       "      <td>0.021188</td>\n",
       "      <td>0.025071</td>\n",
       "      <td>0.218805</td>\n",
       "      <td>0.028679</td>\n",
       "      <td>0.052006</td>\n",
       "      <td>0.043262</td>\n",
       "      <td>0.385833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053756</td>\n",
       "      <td>-0.028285</td>\n",
       "      <td>0.062971</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>-0.009136</td>\n",
       "      <td>0.051093</td>\n",
       "      <td>0.096702</td>\n",
       "      <td>0.082537</td>\n",
       "      <td>0.094164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>IUD</td>\n",
       "      <td>0.279429</td>\n",
       "      <td>0.032460</td>\n",
       "      <td>-0.010758</td>\n",
       "      <td>0.204501</td>\n",
       "      <td>-0.044870</td>\n",
       "      <td>0.027492</td>\n",
       "      <td>0.008226</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.110677</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015819</td>\n",
       "      <td>0.035791</td>\n",
       "      <td>0.117166</td>\n",
       "      <td>0.043708</td>\n",
       "      <td>0.062142</td>\n",
       "      <td>0.135778</td>\n",
       "      <td>0.052108</td>\n",
       "      <td>0.096089</td>\n",
       "      <td>0.013292</td>\n",
       "      <td>0.059231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>IUD (years)</td>\n",
       "      <td>0.215427</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>-0.017163</td>\n",
       "      <td>0.149719</td>\n",
       "      <td>-0.030442</td>\n",
       "      <td>0.038061</td>\n",
       "      <td>0.016292</td>\n",
       "      <td>-0.056548</td>\n",
       "      <td>0.013782</td>\n",
       "      <td>0.749288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011853</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.103148</td>\n",
       "      <td>0.008887</td>\n",
       "      <td>0.035869</td>\n",
       "      <td>0.100340</td>\n",
       "      <td>0.014132</td>\n",
       "      <td>0.087032</td>\n",
       "      <td>0.007103</td>\n",
       "      <td>0.038176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs</td>\n",
       "      <td>-0.084916</td>\n",
       "      <td>0.029117</td>\n",
       "      <td>-0.073419</td>\n",
       "      <td>-0.023108</td>\n",
       "      <td>0.107566</td>\n",
       "      <td>0.057778</td>\n",
       "      <td>0.009123</td>\n",
       "      <td>0.163352</td>\n",
       "      <td>-0.084904</td>\n",
       "      <td>-0.055712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092512</td>\n",
       "      <td>0.553297</td>\n",
       "      <td>-0.036857</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>-0.036857</td>\n",
       "      <td>-0.036971</td>\n",
       "      <td>-0.021617</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.007262</td>\n",
       "      <td>0.025565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs (number)</td>\n",
       "      <td>-0.001330</td>\n",
       "      <td>0.041459</td>\n",
       "      <td>0.016272</td>\n",
       "      <td>0.010603</td>\n",
       "      <td>0.120090</td>\n",
       "      <td>0.088605</td>\n",
       "      <td>0.030247</td>\n",
       "      <td>-0.053642</td>\n",
       "      <td>-0.000444</td>\n",
       "      <td>0.060591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077165</td>\n",
       "      <td>0.898446</td>\n",
       "      <td>-0.012141</td>\n",
       "      <td>-0.008539</td>\n",
       "      <td>-0.012141</td>\n",
       "      <td>-0.022972</td>\n",
       "      <td>0.073186</td>\n",
       "      <td>0.129649</td>\n",
       "      <td>0.061689</td>\n",
       "      <td>0.103153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs:condylomatosis</td>\n",
       "      <td>-0.013751</td>\n",
       "      <td>0.036925</td>\n",
       "      <td>0.034476</td>\n",
       "      <td>-0.031449</td>\n",
       "      <td>0.072711</td>\n",
       "      <td>0.043504</td>\n",
       "      <td>0.007599</td>\n",
       "      <td>-0.025116</td>\n",
       "      <td>0.013985</td>\n",
       "      <td>0.084794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011238</td>\n",
       "      <td>0.701701</td>\n",
       "      <td>-0.034034</td>\n",
       "      <td>-0.023938</td>\n",
       "      <td>-0.034034</td>\n",
       "      <td>-0.039440</td>\n",
       "      <td>0.058905</td>\n",
       "      <td>0.116795</td>\n",
       "      <td>0.065725</td>\n",
       "      <td>0.090164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs:cervical condylomatosis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs:vaginal condylomatosis</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>-0.042120</td>\n",
       "      <td>0.073697</td>\n",
       "      <td>-0.000370</td>\n",
       "      <td>0.063993</td>\n",
       "      <td>0.114655</td>\n",
       "      <td>0.041939</td>\n",
       "      <td>-0.064390</td>\n",
       "      <td>-0.034274</td>\n",
       "      <td>0.035484</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003308</td>\n",
       "      <td>0.206557</td>\n",
       "      <td>-0.010018</td>\n",
       "      <td>-0.007046</td>\n",
       "      <td>-0.010018</td>\n",
       "      <td>-0.011610</td>\n",
       "      <td>-0.014114</td>\n",
       "      <td>-0.021026</td>\n",
       "      <td>-0.015912</td>\n",
       "      <td>-0.017911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs:vulvo-perineal condylomatosis</td>\n",
       "      <td>-0.011499</td>\n",
       "      <td>0.038992</td>\n",
       "      <td>0.038673</td>\n",
       "      <td>-0.030810</td>\n",
       "      <td>0.075825</td>\n",
       "      <td>0.045561</td>\n",
       "      <td>0.008761</td>\n",
       "      <td>-0.029001</td>\n",
       "      <td>0.015690</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011103</td>\n",
       "      <td>0.693256</td>\n",
       "      <td>-0.033624</td>\n",
       "      <td>-0.023650</td>\n",
       "      <td>-0.033624</td>\n",
       "      <td>-0.038965</td>\n",
       "      <td>0.060651</td>\n",
       "      <td>0.119714</td>\n",
       "      <td>0.067686</td>\n",
       "      <td>0.092548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs:syphilis</td>\n",
       "      <td>0.017457</td>\n",
       "      <td>0.028646</td>\n",
       "      <td>-0.095937</td>\n",
       "      <td>0.147318</td>\n",
       "      <td>0.092340</td>\n",
       "      <td>0.013850</td>\n",
       "      <td>-0.003754</td>\n",
       "      <td>-0.006252</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>-0.020393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007076</td>\n",
       "      <td>0.414913</td>\n",
       "      <td>-0.021429</td>\n",
       "      <td>-0.015072</td>\n",
       "      <td>-0.021429</td>\n",
       "      <td>-0.024832</td>\n",
       "      <td>0.010925</td>\n",
       "      <td>0.012965</td>\n",
       "      <td>-0.034034</td>\n",
       "      <td>-0.038311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs:pelvic inflammatory disease</td>\n",
       "      <td>0.024854</td>\n",
       "      <td>0.030929</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.055077</td>\n",
       "      <td>-0.014826</td>\n",
       "      <td>-0.010111</td>\n",
       "      <td>-0.006901</td>\n",
       "      <td>0.023085</td>\n",
       "      <td>-0.012316</td>\n",
       "      <td>-0.011179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001651</td>\n",
       "      <td>0.103097</td>\n",
       "      <td>-0.005000</td>\n",
       "      <td>-0.003517</td>\n",
       "      <td>-0.005000</td>\n",
       "      <td>-0.005795</td>\n",
       "      <td>-0.007044</td>\n",
       "      <td>-0.010495</td>\n",
       "      <td>-0.007942</td>\n",
       "      <td>-0.008940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs:genital herpes</td>\n",
       "      <td>-0.027433</td>\n",
       "      <td>-0.031413</td>\n",
       "      <td>0.024542</td>\n",
       "      <td>-0.030681</td>\n",
       "      <td>-0.014826</td>\n",
       "      <td>-0.010111</td>\n",
       "      <td>-0.006901</td>\n",
       "      <td>0.023085</td>\n",
       "      <td>-0.017107</td>\n",
       "      <td>-0.011179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001651</td>\n",
       "      <td>0.103097</td>\n",
       "      <td>-0.005000</td>\n",
       "      <td>-0.003517</td>\n",
       "      <td>-0.005000</td>\n",
       "      <td>-0.005795</td>\n",
       "      <td>-0.007044</td>\n",
       "      <td>-0.010495</td>\n",
       "      <td>-0.007942</td>\n",
       "      <td>0.130523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs:molluscum contagiosum</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.030929</td>\n",
       "      <td>-0.012185</td>\n",
       "      <td>0.042509</td>\n",
       "      <td>-0.014826</td>\n",
       "      <td>-0.010111</td>\n",
       "      <td>-0.006901</td>\n",
       "      <td>-0.050547</td>\n",
       "      <td>-0.019502</td>\n",
       "      <td>-0.011179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001651</td>\n",
       "      <td>0.103097</td>\n",
       "      <td>-0.005000</td>\n",
       "      <td>-0.003517</td>\n",
       "      <td>-0.005000</td>\n",
       "      <td>-0.005795</td>\n",
       "      <td>-0.007044</td>\n",
       "      <td>-0.010495</td>\n",
       "      <td>-0.007942</td>\n",
       "      <td>-0.008940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs:AIDS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs:HIV</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>0.018752</td>\n",
       "      <td>-0.008499</td>\n",
       "      <td>0.013729</td>\n",
       "      <td>0.070073</td>\n",
       "      <td>0.088930</td>\n",
       "      <td>0.053995</td>\n",
       "      <td>-0.076371</td>\n",
       "      <td>-0.035669</td>\n",
       "      <td>0.007118</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007076</td>\n",
       "      <td>0.549393</td>\n",
       "      <td>-0.021429</td>\n",
       "      <td>0.064753</td>\n",
       "      <td>-0.021429</td>\n",
       "      <td>0.024488</td>\n",
       "      <td>0.093151</td>\n",
       "      <td>0.128842</td>\n",
       "      <td>0.076576</td>\n",
       "      <td>0.127702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs:Hepatitis B</td>\n",
       "      <td>-0.027433</td>\n",
       "      <td>-0.010633</td>\n",
       "      <td>0.012299</td>\n",
       "      <td>-0.030681</td>\n",
       "      <td>0.078706</td>\n",
       "      <td>0.099313</td>\n",
       "      <td>0.101342</td>\n",
       "      <td>-0.050547</td>\n",
       "      <td>-0.019502</td>\n",
       "      <td>-0.011179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001651</td>\n",
       "      <td>0.103097</td>\n",
       "      <td>-0.005000</td>\n",
       "      <td>-0.003517</td>\n",
       "      <td>-0.005000</td>\n",
       "      <td>-0.005795</td>\n",
       "      <td>-0.007044</td>\n",
       "      <td>-0.010495</td>\n",
       "      <td>-0.007942</td>\n",
       "      <td>-0.008940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs:HPV</td>\n",
       "      <td>0.040861</td>\n",
       "      <td>0.014360</td>\n",
       "      <td>0.034728</td>\n",
       "      <td>-0.026153</td>\n",
       "      <td>0.045197</td>\n",
       "      <td>0.051201</td>\n",
       "      <td>-0.008015</td>\n",
       "      <td>0.032666</td>\n",
       "      <td>0.053756</td>\n",
       "      <td>-0.015819</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.065957</td>\n",
       "      <td>0.330203</td>\n",
       "      <td>-0.004977</td>\n",
       "      <td>0.330203</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>-0.009968</td>\n",
       "      <td>-0.014850</td>\n",
       "      <td>-0.011238</td>\n",
       "      <td>-0.012650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STDs: Number of diagnosis</td>\n",
       "      <td>-0.001606</td>\n",
       "      <td>0.053056</td>\n",
       "      <td>-0.013331</td>\n",
       "      <td>0.034912</td>\n",
       "      <td>0.117277</td>\n",
       "      <td>0.078303</td>\n",
       "      <td>0.029912</td>\n",
       "      <td>-0.062199</td>\n",
       "      <td>-0.028285</td>\n",
       "      <td>0.035791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065957</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015423</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>-0.015423</td>\n",
       "      <td>-0.002289</td>\n",
       "      <td>0.076787</td>\n",
       "      <td>0.130873</td>\n",
       "      <td>0.055114</td>\n",
       "      <td>0.097449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Dx:Cancer</td>\n",
       "      <td>0.110340</td>\n",
       "      <td>0.023699</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>0.036962</td>\n",
       "      <td>0.003270</td>\n",
       "      <td>0.052859</td>\n",
       "      <td>0.107229</td>\n",
       "      <td>0.011278</td>\n",
       "      <td>0.062971</td>\n",
       "      <td>0.117166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330203</td>\n",
       "      <td>-0.015423</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015072</td>\n",
       "      <td>0.886508</td>\n",
       "      <td>0.665647</td>\n",
       "      <td>0.134264</td>\n",
       "      <td>0.157812</td>\n",
       "      <td>0.113446</td>\n",
       "      <td>0.160905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Dx:CIN</td>\n",
       "      <td>0.061443</td>\n",
       "      <td>0.016669</td>\n",
       "      <td>-0.032628</td>\n",
       "      <td>-0.002600</td>\n",
       "      <td>-0.044686</td>\n",
       "      <td>-0.030476</td>\n",
       "      <td>-0.020800</td>\n",
       "      <td>-0.004397</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.043708</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004977</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>-0.015072</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015072</td>\n",
       "      <td>0.606939</td>\n",
       "      <td>-0.021233</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>-0.023938</td>\n",
       "      <td>0.113172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Dx:HPV</td>\n",
       "      <td>0.101722</td>\n",
       "      <td>0.028646</td>\n",
       "      <td>0.043964</td>\n",
       "      <td>0.048578</td>\n",
       "      <td>0.025538</td>\n",
       "      <td>0.055398</td>\n",
       "      <td>0.109118</td>\n",
       "      <td>0.028808</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.062142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330203</td>\n",
       "      <td>-0.015423</td>\n",
       "      <td>0.886508</td>\n",
       "      <td>-0.015072</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.616327</td>\n",
       "      <td>0.134264</td>\n",
       "      <td>0.157812</td>\n",
       "      <td>0.113446</td>\n",
       "      <td>0.160905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Dx</td>\n",
       "      <td>0.092635</td>\n",
       "      <td>0.024597</td>\n",
       "      <td>0.035748</td>\n",
       "      <td>0.014227</td>\n",
       "      <td>-0.054271</td>\n",
       "      <td>-0.050213</td>\n",
       "      <td>-0.034270</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>-0.009136</td>\n",
       "      <td>0.135778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>-0.002289</td>\n",
       "      <td>0.665647</td>\n",
       "      <td>0.606939</td>\n",
       "      <td>0.616327</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.072215</td>\n",
       "      <td>0.098952</td>\n",
       "      <td>0.088740</td>\n",
       "      <td>0.157607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Hinselmann</td>\n",
       "      <td>-0.003967</td>\n",
       "      <td>-0.039098</td>\n",
       "      <td>-0.016549</td>\n",
       "      <td>0.037809</td>\n",
       "      <td>0.039562</td>\n",
       "      <td>0.070352</td>\n",
       "      <td>0.026086</td>\n",
       "      <td>0.012360</td>\n",
       "      <td>0.051093</td>\n",
       "      <td>0.052108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009968</td>\n",
       "      <td>0.076787</td>\n",
       "      <td>0.134264</td>\n",
       "      <td>-0.021233</td>\n",
       "      <td>0.134264</td>\n",
       "      <td>0.072215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.650249</td>\n",
       "      <td>0.192467</td>\n",
       "      <td>0.547417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Schiller</td>\n",
       "      <td>0.103283</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>0.085810</td>\n",
       "      <td>0.059913</td>\n",
       "      <td>0.093479</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>-0.034002</td>\n",
       "      <td>0.096702</td>\n",
       "      <td>0.096089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014850</td>\n",
       "      <td>0.130873</td>\n",
       "      <td>0.157812</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>0.157812</td>\n",
       "      <td>0.098952</td>\n",
       "      <td>0.650249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.361486</td>\n",
       "      <td>0.733204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Citology</td>\n",
       "      <td>-0.016862</td>\n",
       "      <td>0.024067</td>\n",
       "      <td>-0.010974</td>\n",
       "      <td>-0.027675</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>-0.007275</td>\n",
       "      <td>0.004250</td>\n",
       "      <td>-0.025116</td>\n",
       "      <td>0.082537</td>\n",
       "      <td>0.013292</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011238</td>\n",
       "      <td>0.055114</td>\n",
       "      <td>0.113446</td>\n",
       "      <td>-0.023938</td>\n",
       "      <td>0.113446</td>\n",
       "      <td>0.088740</td>\n",
       "      <td>0.192467</td>\n",
       "      <td>0.361486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.327466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Biopsy</td>\n",
       "      <td>0.055956</td>\n",
       "      <td>-0.000408</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>0.040215</td>\n",
       "      <td>0.029733</td>\n",
       "      <td>0.061204</td>\n",
       "      <td>0.024487</td>\n",
       "      <td>-0.018015</td>\n",
       "      <td>0.094164</td>\n",
       "      <td>0.059231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012650</td>\n",
       "      <td>0.097449</td>\n",
       "      <td>0.160905</td>\n",
       "      <td>0.113172</td>\n",
       "      <td>0.160905</td>\n",
       "      <td>0.157607</td>\n",
       "      <td>0.547417</td>\n",
       "      <td>0.733204</td>\n",
       "      <td>0.327466</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Age  Number of sexual partners  \\\n",
       "Age                                 1.000000                   0.085971   \n",
       "Number of sexual partners           0.085971                   1.000000   \n",
       "First sexual intercourse            0.369175                  -0.145847   \n",
       "Num of pregnancies                  0.525892                   0.077439   \n",
       "Smokes                              0.045244                   0.243479   \n",
       "Smokes (years)                      0.218261                   0.175729   \n",
       "Smokes (packs/year)                 0.131861                   0.174968   \n",
       "Hormonal Contraceptives             0.029201                   0.004027   \n",
       "Hormonal Contraceptives (years)     0.295267                   0.021188   \n",
       "IUD                                 0.279429                   0.032460   \n",
       "IUD (years)                         0.215427                   0.006252   \n",
       "STDs                               -0.084916                   0.029117   \n",
       "STDs (number)                      -0.001330                   0.041459   \n",
       "STDs:condylomatosis                -0.013751                   0.036925   \n",
       "STDs:cervical condylomatosis             NaN                        NaN   \n",
       "STDs:vaginal condylomatosis         0.009505                  -0.042120   \n",
       "STDs:vulvo-perineal condylomatosis -0.011499                   0.038992   \n",
       "STDs:syphilis                       0.017457                   0.028646   \n",
       "STDs:pelvic inflammatory disease    0.024854                   0.030929   \n",
       "STDs:genital herpes                -0.027433                  -0.031413   \n",
       "STDs:molluscum contagiosum          0.000722                   0.030929   \n",
       "STDs:AIDS                                NaN                        NaN   \n",
       "STDs:HIV                            0.005009                   0.018752   \n",
       "STDs:Hepatitis B                   -0.027433                  -0.010633   \n",
       "STDs:HPV                            0.040861                   0.014360   \n",
       "STDs: Number of diagnosis          -0.001606                   0.053056   \n",
       "Dx:Cancer                           0.110340                   0.023699   \n",
       "Dx:CIN                              0.061443                   0.016669   \n",
       "Dx:HPV                              0.101722                   0.028646   \n",
       "Dx                                  0.092635                   0.024597   \n",
       "Hinselmann                         -0.003967                  -0.039098   \n",
       "Schiller                            0.103283                  -0.007230   \n",
       "Citology                           -0.016862                   0.024067   \n",
       "Biopsy                              0.055956                  -0.000408   \n",
       "\n",
       "                                    First sexual intercourse  \\\n",
       "Age                                                 0.369175   \n",
       "Number of sexual partners                          -0.145847   \n",
       "First sexual intercourse                            1.000000   \n",
       "Num of pregnancies                                 -0.056374   \n",
       "Smokes                                             -0.119365   \n",
       "Smokes (years)                                     -0.058207   \n",
       "Smokes (packs/year)                                -0.056232   \n",
       "Hormonal Contraceptives                            -0.009232   \n",
       "Hormonal Contraceptives (years)                     0.025071   \n",
       "IUD                                                -0.010758   \n",
       "IUD (years)                                        -0.017163   \n",
       "STDs                                               -0.073419   \n",
       "STDs (number)                                       0.016272   \n",
       "STDs:condylomatosis                                 0.034476   \n",
       "STDs:cervical condylomatosis                             NaN   \n",
       "STDs:vaginal condylomatosis                         0.073697   \n",
       "STDs:vulvo-perineal condylomatosis                  0.038673   \n",
       "STDs:syphilis                                      -0.095937   \n",
       "STDs:pelvic inflammatory disease                    0.000057   \n",
       "STDs:genital herpes                                 0.024542   \n",
       "STDs:molluscum contagiosum                         -0.012185   \n",
       "STDs:AIDS                                                NaN   \n",
       "STDs:HIV                                           -0.008499   \n",
       "STDs:Hepatitis B                                    0.012299   \n",
       "STDs:HPV                                            0.034728   \n",
       "STDs: Number of diagnosis                          -0.013331   \n",
       "Dx:Cancer                                           0.067281   \n",
       "Dx:CIN                                             -0.032628   \n",
       "Dx:HPV                                              0.043964   \n",
       "Dx                                                  0.035748   \n",
       "Hinselmann                                         -0.016549   \n",
       "Schiller                                            0.003489   \n",
       "Citology                                           -0.010974   \n",
       "Biopsy                                              0.007259   \n",
       "\n",
       "                                    Num of pregnancies    Smokes  \\\n",
       "Age                                           0.525892  0.045244   \n",
       "Number of sexual partners                     0.077439  0.243479   \n",
       "First sexual intercourse                     -0.056374 -0.119365   \n",
       "Num of pregnancies                            1.000000  0.068307   \n",
       "Smokes                                        0.068307  1.000000   \n",
       "Smokes (years)                                0.175832  0.682009   \n",
       "Smokes (packs/year)                           0.096976  0.465475   \n",
       "Hormonal Contraceptives                       0.118938 -0.002485   \n",
       "Hormonal Contraceptives (years)               0.218805  0.028679   \n",
       "IUD                                           0.204501 -0.044870   \n",
       "IUD (years)                                   0.149719 -0.030442   \n",
       "STDs                                         -0.023108  0.107566   \n",
       "STDs (number)                                 0.010603  0.120090   \n",
       "STDs:condylomatosis                          -0.031449  0.072711   \n",
       "STDs:cervical condylomatosis                       NaN       NaN   \n",
       "STDs:vaginal condylomatosis                  -0.000370  0.063993   \n",
       "STDs:vulvo-perineal condylomatosis           -0.030810  0.075825   \n",
       "STDs:syphilis                                 0.147318  0.092340   \n",
       "STDs:pelvic inflammatory disease             -0.055077 -0.014826   \n",
       "STDs:genital herpes                          -0.030681 -0.014826   \n",
       "STDs:molluscum contagiosum                    0.042509 -0.014826   \n",
       "STDs:AIDS                                          NaN       NaN   \n",
       "STDs:HIV                                      0.013729  0.070073   \n",
       "STDs:Hepatitis B                             -0.030681  0.078706   \n",
       "STDs:HPV                                     -0.026153  0.045197   \n",
       "STDs: Number of diagnosis                     0.034912  0.117277   \n",
       "Dx:Cancer                                     0.036962  0.003270   \n",
       "Dx:CIN                                       -0.002600 -0.044686   \n",
       "Dx:HPV                                        0.048578  0.025538   \n",
       "Dx                                            0.014227 -0.054271   \n",
       "Hinselmann                                    0.037809  0.039562   \n",
       "Schiller                                      0.085810  0.059913   \n",
       "Citology                                     -0.027675  0.000371   \n",
       "Biopsy                                        0.040215  0.029733   \n",
       "\n",
       "                                    Smokes (years)  Smokes (packs/year)  \\\n",
       "Age                                       0.218261             0.131861   \n",
       "Number of sexual partners                 0.175729             0.174968   \n",
       "First sexual intercourse                 -0.058207            -0.056232   \n",
       "Num of pregnancies                        0.175832             0.096976   \n",
       "Smokes                                    0.682009             0.465475   \n",
       "Smokes (years)                            1.000000             0.724320   \n",
       "Smokes (packs/year)                       0.724320             1.000000   \n",
       "Hormonal Contraceptives                  -0.013888             0.001713   \n",
       "Hormonal Contraceptives (years)           0.052006             0.043262   \n",
       "IUD                                       0.027492             0.008226   \n",
       "IUD (years)                               0.038061             0.016292   \n",
       "STDs                                      0.057778             0.009123   \n",
       "STDs (number)                             0.088605             0.030247   \n",
       "STDs:condylomatosis                       0.043504             0.007599   \n",
       "STDs:cervical condylomatosis                   NaN                  NaN   \n",
       "STDs:vaginal condylomatosis               0.114655             0.041939   \n",
       "STDs:vulvo-perineal condylomatosis        0.045561             0.008761   \n",
       "STDs:syphilis                             0.013850            -0.003754   \n",
       "STDs:pelvic inflammatory disease         -0.010111            -0.006901   \n",
       "STDs:genital herpes                      -0.010111            -0.006901   \n",
       "STDs:molluscum contagiosum               -0.010111            -0.006901   \n",
       "STDs:AIDS                                      NaN                  NaN   \n",
       "STDs:HIV                                  0.088930             0.053995   \n",
       "STDs:Hepatitis B                          0.099313             0.101342   \n",
       "STDs:HPV                                  0.051201            -0.008015   \n",
       "STDs: Number of diagnosis                 0.078303             0.029912   \n",
       "Dx:Cancer                                 0.052859             0.107229   \n",
       "Dx:CIN                                   -0.030476            -0.020800   \n",
       "Dx:HPV                                    0.055398             0.109118   \n",
       "Dx                                       -0.050213            -0.034270   \n",
       "Hinselmann                                0.070352             0.026086   \n",
       "Schiller                                  0.093479             0.017200   \n",
       "Citology                                 -0.007275             0.004250   \n",
       "Biopsy                                    0.061204             0.024487   \n",
       "\n",
       "                                    Hormonal Contraceptives  \\\n",
       "Age                                                0.029201   \n",
       "Number of sexual partners                          0.004027   \n",
       "First sexual intercourse                          -0.009232   \n",
       "Num of pregnancies                                 0.118938   \n",
       "Smokes                                            -0.002485   \n",
       "Smokes (years)                                    -0.013888   \n",
       "Smokes (packs/year)                                0.001713   \n",
       "Hormonal Contraceptives                            1.000000   \n",
       "Hormonal Contraceptives (years)                    0.385833   \n",
       "IUD                                                0.000188   \n",
       "IUD (years)                                       -0.056548   \n",
       "STDs                                               0.163352   \n",
       "STDs (number)                                     -0.053642   \n",
       "STDs:condylomatosis                               -0.025116   \n",
       "STDs:cervical condylomatosis                            NaN   \n",
       "STDs:vaginal condylomatosis                       -0.064390   \n",
       "STDs:vulvo-perineal condylomatosis                -0.029001   \n",
       "STDs:syphilis                                     -0.006252   \n",
       "STDs:pelvic inflammatory disease                   0.023085   \n",
       "STDs:genital herpes                                0.023085   \n",
       "STDs:molluscum contagiosum                        -0.050547   \n",
       "STDs:AIDS                                               NaN   \n",
       "STDs:HIV                                          -0.076371   \n",
       "STDs:Hepatitis B                                  -0.050547   \n",
       "STDs:HPV                                           0.032666   \n",
       "STDs: Number of diagnosis                         -0.062199   \n",
       "Dx:Cancer                                          0.011278   \n",
       "Dx:CIN                                            -0.004397   \n",
       "Dx:HPV                                             0.028808   \n",
       "Dx                                                -0.007245   \n",
       "Hinselmann                                         0.012360   \n",
       "Schiller                                          -0.034002   \n",
       "Citology                                          -0.025116   \n",
       "Biopsy                                            -0.018015   \n",
       "\n",
       "                                    Hormonal Contraceptives (years)       IUD  \\\n",
       "Age                                                        0.295267  0.279429   \n",
       "Number of sexual partners                                  0.021188  0.032460   \n",
       "First sexual intercourse                                   0.025071 -0.010758   \n",
       "Num of pregnancies                                         0.218805  0.204501   \n",
       "Smokes                                                     0.028679 -0.044870   \n",
       "Smokes (years)                                             0.052006  0.027492   \n",
       "Smokes (packs/year)                                        0.043262  0.008226   \n",
       "Hormonal Contraceptives                                    0.385833  0.000188   \n",
       "Hormonal Contraceptives (years)                            1.000000  0.110677   \n",
       "IUD                                                        0.110677  1.000000   \n",
       "IUD (years)                                                0.013782  0.749288   \n",
       "STDs                                                      -0.084904 -0.055712   \n",
       "STDs (number)                                             -0.000444  0.060591   \n",
       "STDs:condylomatosis                                        0.013985  0.084794   \n",
       "STDs:cervical condylomatosis                                    NaN       NaN   \n",
       "STDs:vaginal condylomatosis                               -0.034274  0.035484   \n",
       "STDs:vulvo-perineal condylomatosis                         0.015690  0.069399   \n",
       "STDs:syphilis                                              0.003112 -0.020393   \n",
       "STDs:pelvic inflammatory disease                          -0.012316 -0.011179   \n",
       "STDs:genital herpes                                       -0.017107 -0.011179   \n",
       "STDs:molluscum contagiosum                                -0.019502 -0.011179   \n",
       "STDs:AIDS                                                       NaN       NaN   \n",
       "STDs:HIV                                                  -0.035669  0.007118   \n",
       "STDs:Hepatitis B                                          -0.019502 -0.011179   \n",
       "STDs:HPV                                                   0.053756 -0.015819   \n",
       "STDs: Number of diagnosis                                 -0.028285  0.035791   \n",
       "Dx:Cancer                                                  0.062971  0.117166   \n",
       "Dx:CIN                                                     0.003793  0.043708   \n",
       "Dx:HPV                                                     0.065640  0.062142   \n",
       "Dx                                                        -0.009136  0.135778   \n",
       "Hinselmann                                                 0.051093  0.052108   \n",
       "Schiller                                                   0.096702  0.096089   \n",
       "Citology                                                   0.082537  0.013292   \n",
       "Biopsy                                                     0.094164  0.059231   \n",
       "\n",
       "                                    ...  STDs:HPV  STDs: Number of diagnosis  \\\n",
       "Age                                 ...  0.040861                  -0.001606   \n",
       "Number of sexual partners           ...  0.014360                   0.053056   \n",
       "First sexual intercourse            ...  0.034728                  -0.013331   \n",
       "Num of pregnancies                  ... -0.026153                   0.034912   \n",
       "Smokes                              ...  0.045197                   0.117277   \n",
       "Smokes (years)                      ...  0.051201                   0.078303   \n",
       "Smokes (packs/year)                 ... -0.008015                   0.029912   \n",
       "Hormonal Contraceptives             ...  0.032666                  -0.062199   \n",
       "Hormonal Contraceptives (years)     ...  0.053756                  -0.028285   \n",
       "IUD                                 ... -0.015819                   0.035791   \n",
       "IUD (years)                         ... -0.011853                   0.012191   \n",
       "STDs                                ...  0.092512                   0.553297   \n",
       "STDs (number)                       ...  0.077165                   0.898446   \n",
       "STDs:condylomatosis                 ... -0.011238                   0.701701   \n",
       "STDs:cervical condylomatosis        ...       NaN                        NaN   \n",
       "STDs:vaginal condylomatosis         ... -0.003308                   0.206557   \n",
       "STDs:vulvo-perineal condylomatosis  ... -0.011103                   0.693256   \n",
       "STDs:syphilis                       ... -0.007076                   0.414913   \n",
       "STDs:pelvic inflammatory disease    ... -0.001651                   0.103097   \n",
       "STDs:genital herpes                 ... -0.001651                   0.103097   \n",
       "STDs:molluscum contagiosum          ... -0.001651                   0.103097   \n",
       "STDs:AIDS                           ...       NaN                        NaN   \n",
       "STDs:HIV                            ... -0.007076                   0.549393   \n",
       "STDs:Hepatitis B                    ... -0.001651                   0.103097   \n",
       "STDs:HPV                            ...  1.000000                   0.065957   \n",
       "STDs: Number of diagnosis           ...  0.065957                   1.000000   \n",
       "Dx:Cancer                           ...  0.330203                  -0.015423   \n",
       "Dx:CIN                              ... -0.004977                   0.008070   \n",
       "Dx:HPV                              ...  0.330203                  -0.015423   \n",
       "Dx                                  ...  0.138371                  -0.002289   \n",
       "Hinselmann                          ... -0.009968                   0.076787   \n",
       "Schiller                            ... -0.014850                   0.130873   \n",
       "Citology                            ... -0.011238                   0.055114   \n",
       "Biopsy                              ... -0.012650                   0.097449   \n",
       "\n",
       "                                    Dx:Cancer    Dx:CIN    Dx:HPV        Dx  \\\n",
       "Age                                  0.110340  0.061443  0.101722  0.092635   \n",
       "Number of sexual partners            0.023699  0.016669  0.028646  0.024597   \n",
       "First sexual intercourse             0.067281 -0.032628  0.043964  0.035748   \n",
       "Num of pregnancies                   0.036962 -0.002600  0.048578  0.014227   \n",
       "Smokes                               0.003270 -0.044686  0.025538 -0.054271   \n",
       "Smokes (years)                       0.052859 -0.030476  0.055398 -0.050213   \n",
       "Smokes (packs/year)                  0.107229 -0.020800  0.109118 -0.034270   \n",
       "Hormonal Contraceptives              0.011278 -0.004397  0.028808 -0.007245   \n",
       "Hormonal Contraceptives (years)      0.062971  0.003793  0.065640 -0.009136   \n",
       "IUD                                  0.117166  0.043708  0.062142  0.135778   \n",
       "IUD (years)                          0.103148  0.008887  0.035869  0.100340   \n",
       "STDs                                -0.036857  0.001949 -0.036857 -0.036971   \n",
       "STDs (number)                       -0.012141 -0.008539 -0.012141 -0.022972   \n",
       "STDs:condylomatosis                 -0.034034 -0.023938 -0.034034 -0.039440   \n",
       "STDs:cervical condylomatosis              NaN       NaN       NaN       NaN   \n",
       "STDs:vaginal condylomatosis         -0.010018 -0.007046 -0.010018 -0.011610   \n",
       "STDs:vulvo-perineal condylomatosis  -0.033624 -0.023650 -0.033624 -0.038965   \n",
       "STDs:syphilis                       -0.021429 -0.015072 -0.021429 -0.024832   \n",
       "STDs:pelvic inflammatory disease    -0.005000 -0.003517 -0.005000 -0.005795   \n",
       "STDs:genital herpes                 -0.005000 -0.003517 -0.005000 -0.005795   \n",
       "STDs:molluscum contagiosum          -0.005000 -0.003517 -0.005000 -0.005795   \n",
       "STDs:AIDS                                 NaN       NaN       NaN       NaN   \n",
       "STDs:HIV                            -0.021429  0.064753 -0.021429  0.024488   \n",
       "STDs:Hepatitis B                    -0.005000 -0.003517 -0.005000 -0.005795   \n",
       "STDs:HPV                             0.330203 -0.004977  0.330203  0.138371   \n",
       "STDs: Number of diagnosis           -0.015423  0.008070 -0.015423 -0.002289   \n",
       "Dx:Cancer                            1.000000 -0.015072  0.886508  0.665647   \n",
       "Dx:CIN                              -0.015072  1.000000 -0.015072  0.606939   \n",
       "Dx:HPV                               0.886508 -0.015072  1.000000  0.616327   \n",
       "Dx                                   0.665647  0.606939  0.616327  1.000000   \n",
       "Hinselmann                           0.134264 -0.021233  0.134264  0.072215   \n",
       "Schiller                             0.157812  0.009119  0.157812  0.098952   \n",
       "Citology                             0.113446 -0.023938  0.113446  0.088740   \n",
       "Biopsy                               0.160905  0.113172  0.160905  0.157607   \n",
       "\n",
       "                                    Hinselmann  Schiller  Citology    Biopsy  \n",
       "Age                                  -0.003967  0.103283 -0.016862  0.055956  \n",
       "Number of sexual partners            -0.039098 -0.007230  0.024067 -0.000408  \n",
       "First sexual intercourse             -0.016549  0.003489 -0.010974  0.007259  \n",
       "Num of pregnancies                    0.037809  0.085810 -0.027675  0.040215  \n",
       "Smokes                                0.039562  0.059913  0.000371  0.029733  \n",
       "Smokes (years)                        0.070352  0.093479 -0.007275  0.061204  \n",
       "Smokes (packs/year)                   0.026086  0.017200  0.004250  0.024487  \n",
       "Hormonal Contraceptives               0.012360 -0.034002 -0.025116 -0.018015  \n",
       "Hormonal Contraceptives (years)       0.051093  0.096702  0.082537  0.094164  \n",
       "IUD                                   0.052108  0.096089  0.013292  0.059231  \n",
       "IUD (years)                           0.014132  0.087032  0.007103  0.038176  \n",
       "STDs                                 -0.021617  0.001320  0.007262  0.025565  \n",
       "STDs (number)                         0.073186  0.129649  0.061689  0.103153  \n",
       "STDs:condylomatosis                   0.058905  0.116795  0.065725  0.090164  \n",
       "STDs:cervical condylomatosis               NaN       NaN       NaN       NaN  \n",
       "STDs:vaginal condylomatosis          -0.014114 -0.021026 -0.015912 -0.017911  \n",
       "STDs:vulvo-perineal condylomatosis    0.060651  0.119714  0.067686  0.092548  \n",
       "STDs:syphilis                         0.010925  0.012965 -0.034034 -0.038311  \n",
       "STDs:pelvic inflammatory disease     -0.007044 -0.010495 -0.007942 -0.008940  \n",
       "STDs:genital herpes                  -0.007044 -0.010495 -0.007942  0.130523  \n",
       "STDs:molluscum contagiosum           -0.007044 -0.010495 -0.007942 -0.008940  \n",
       "STDs:AIDS                                  NaN       NaN       NaN       NaN  \n",
       "STDs:HIV                              0.093151  0.128842  0.076576  0.127702  \n",
       "STDs:Hepatitis B                     -0.007044 -0.010495 -0.007942 -0.008940  \n",
       "STDs:HPV                             -0.009968 -0.014850 -0.011238 -0.012650  \n",
       "STDs: Number of diagnosis             0.076787  0.130873  0.055114  0.097449  \n",
       "Dx:Cancer                             0.134264  0.157812  0.113446  0.160905  \n",
       "Dx:CIN                               -0.021233  0.009119 -0.023938  0.113172  \n",
       "Dx:HPV                                0.134264  0.157812  0.113446  0.160905  \n",
       "Dx                                    0.072215  0.098952  0.088740  0.157607  \n",
       "Hinselmann                            1.000000  0.650249  0.192467  0.547417  \n",
       "Schiller                              0.650249  1.000000  0.361486  0.733204  \n",
       "Citology                              0.192467  0.361486  1.000000  0.327466  \n",
       "Biopsy                                0.547417  0.733204  0.327466  1.000000  \n",
       "\n",
       "[34 rows x 34 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "def calc_specificity(y_actual, y_pred, thresh):\n",
    "    # calculates specificity\n",
    "    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
    "def print_report(y_actual, y_pred, thresh):\n",
    "    \n",
    "    auc = roc_auc_score(y_actual, y_pred)\n",
    "    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n",
    "    recall = recall_score(y_actual, (y_pred > thresh))\n",
    "    precision = precision_score(y_actual, (y_pred > thresh))\n",
    "    specificity = calc_specificity(y_actual, y_pred, thresh)\n",
    "    print('AUC:%.3f'%auc)\n",
    "    print('accuracy:%.3f'%accuracy)\n",
    "    print('recall:%.3f'%recall)\n",
    "    print('precision:%.3f'%precision)\n",
    "    print('specificity:%.3f'%specificity)\n",
    "    print(' ')\n",
    "    return auc, accuracy, recall, precision, specificity\n",
    "#setting threshold to 50% since our data is now balanced \n",
    "thresh = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X= df.iloc[:, :33].values\n",
    "y=df.iloc[:, 33].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addressing class imbalance with smote\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=12)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(1000), alpha=0.0001,\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73274263\n",
      "Iteration 2, loss = 0.70793299\n",
      "Iteration 3, loss = 0.67535075\n",
      "Iteration 4, loss = 0.64332804\n",
      "Iteration 5, loss = 0.61385736\n",
      "Iteration 6, loss = 0.58701080\n",
      "Iteration 7, loss = 0.56368133\n",
      "Iteration 8, loss = 0.54284611\n",
      "Iteration 9, loss = 0.52372010\n",
      "Iteration 10, loss = 0.50595975\n",
      "Iteration 11, loss = 0.48971830\n",
      "Iteration 12, loss = 0.47462427\n",
      "Iteration 13, loss = 0.46060812\n",
      "Iteration 14, loss = 0.44753062\n",
      "Iteration 15, loss = 0.43530081\n",
      "Iteration 16, loss = 0.42378002\n",
      "Iteration 17, loss = 0.41297158\n",
      "Iteration 18, loss = 0.40271122\n",
      "Iteration 19, loss = 0.39313115\n",
      "Iteration 20, loss = 0.38397464\n",
      "Iteration 21, loss = 0.37528568\n",
      "Iteration 22, loss = 0.36714487\n",
      "Iteration 23, loss = 0.35943783\n",
      "Iteration 24, loss = 0.35202062\n",
      "Iteration 25, loss = 0.34505675\n",
      "Iteration 26, loss = 0.33831669\n",
      "Iteration 27, loss = 0.33196664\n",
      "Iteration 28, loss = 0.32596652\n",
      "Iteration 29, loss = 0.32012442\n",
      "Iteration 30, loss = 0.31455700\n",
      "Iteration 31, loss = 0.30929042\n",
      "Iteration 32, loss = 0.30419012\n",
      "Iteration 33, loss = 0.29940422\n",
      "Iteration 34, loss = 0.29471661\n",
      "Iteration 35, loss = 0.29024279\n",
      "Iteration 36, loss = 0.28593253\n",
      "Iteration 37, loss = 0.28178381\n",
      "Iteration 38, loss = 0.27776643\n",
      "Iteration 39, loss = 0.27398416\n",
      "Iteration 40, loss = 0.27028245\n",
      "Iteration 41, loss = 0.26671869\n",
      "Iteration 42, loss = 0.26327042\n",
      "Iteration 43, loss = 0.26000599\n",
      "Iteration 44, loss = 0.25676072\n",
      "Iteration 45, loss = 0.25374358\n",
      "Iteration 46, loss = 0.25072516\n",
      "Iteration 47, loss = 0.24785792\n",
      "Iteration 48, loss = 0.24511111\n",
      "Iteration 49, loss = 0.24244111\n",
      "Iteration 50, loss = 0.23990963\n",
      "Iteration 51, loss = 0.23735325\n",
      "Iteration 52, loss = 0.23491114\n",
      "Iteration 53, loss = 0.23254383\n",
      "Iteration 54, loss = 0.23024689\n",
      "Iteration 55, loss = 0.22805572\n",
      "Iteration 56, loss = 0.22586858\n",
      "Iteration 57, loss = 0.22377075\n",
      "Iteration 58, loss = 0.22176427\n",
      "Iteration 59, loss = 0.21982384\n",
      "Iteration 60, loss = 0.21790819\n",
      "Iteration 61, loss = 0.21605660\n",
      "Iteration 62, loss = 0.21419389\n",
      "Iteration 63, loss = 0.21244772\n",
      "Iteration 64, loss = 0.21073666\n",
      "Iteration 65, loss = 0.20908586\n",
      "Iteration 66, loss = 0.20747847\n",
      "Iteration 67, loss = 0.20590327\n",
      "Iteration 68, loss = 0.20434082\n",
      "Iteration 69, loss = 0.20284005\n",
      "Iteration 70, loss = 0.20135527\n",
      "Iteration 71, loss = 0.19992933\n",
      "Iteration 72, loss = 0.19854504\n",
      "Iteration 73, loss = 0.19714546\n",
      "Iteration 74, loss = 0.19585898\n",
      "Iteration 75, loss = 0.19454277\n",
      "Iteration 76, loss = 0.19326811\n",
      "Iteration 77, loss = 0.19206201\n",
      "Iteration 78, loss = 0.19083243\n",
      "Iteration 79, loss = 0.18968495\n",
      "Iteration 80, loss = 0.18853303\n",
      "Iteration 81, loss = 0.18741055\n",
      "Iteration 82, loss = 0.18633245\n",
      "Iteration 83, loss = 0.18520636\n",
      "Iteration 84, loss = 0.18414127\n",
      "Iteration 85, loss = 0.18312515\n",
      "Iteration 86, loss = 0.18211446\n",
      "Iteration 87, loss = 0.18106705\n",
      "Iteration 88, loss = 0.18009341\n",
      "Iteration 89, loss = 0.17913379\n",
      "Iteration 90, loss = 0.17820353\n",
      "Iteration 91, loss = 0.17723985\n",
      "Iteration 92, loss = 0.17634423\n",
      "Iteration 93, loss = 0.17541454\n",
      "Iteration 94, loss = 0.17454317\n",
      "Iteration 95, loss = 0.17367626\n",
      "Iteration 96, loss = 0.17280662\n",
      "Iteration 97, loss = 0.17200272\n",
      "Iteration 98, loss = 0.17116092\n",
      "Iteration 99, loss = 0.17035773\n",
      "Iteration 100, loss = 0.16957305\n",
      "Iteration 101, loss = 0.16877776\n",
      "Iteration 102, loss = 0.16801854\n",
      "Iteration 103, loss = 0.16729460\n",
      "Iteration 104, loss = 0.16655351\n",
      "Iteration 105, loss = 0.16581055\n",
      "Iteration 106, loss = 0.16510628\n",
      "Iteration 107, loss = 0.16441312\n",
      "Iteration 108, loss = 0.16374591\n",
      "Iteration 109, loss = 0.16303637\n",
      "Iteration 110, loss = 0.16237772\n",
      "Iteration 111, loss = 0.16172101\n",
      "Iteration 112, loss = 0.16109709\n",
      "Iteration 113, loss = 0.16045568\n",
      "Iteration 114, loss = 0.15982912\n",
      "Iteration 115, loss = 0.15921976\n",
      "Iteration 116, loss = 0.15862433\n",
      "Iteration 117, loss = 0.15802021\n",
      "Iteration 118, loss = 0.15746406\n",
      "Iteration 119, loss = 0.15685510\n",
      "Iteration 120, loss = 0.15629501\n",
      "Iteration 121, loss = 0.15572205\n",
      "Iteration 122, loss = 0.15516077\n",
      "Iteration 123, loss = 0.15461565\n",
      "Iteration 124, loss = 0.15406571\n",
      "Iteration 125, loss = 0.15352320\n",
      "Iteration 126, loss = 0.15297307\n",
      "Iteration 127, loss = 0.15245728\n",
      "Iteration 128, loss = 0.15194025\n",
      "Iteration 129, loss = 0.15140905\n",
      "Iteration 130, loss = 0.15092191\n",
      "Iteration 131, loss = 0.15038832\n",
      "Iteration 132, loss = 0.14993535\n",
      "Iteration 133, loss = 0.14941174\n",
      "Iteration 134, loss = 0.14895364\n",
      "Iteration 135, loss = 0.14846540\n",
      "Iteration 136, loss = 0.14799561\n",
      "Iteration 137, loss = 0.14754028\n",
      "Iteration 138, loss = 0.14707427\n",
      "Iteration 139, loss = 0.14661782\n",
      "Iteration 140, loss = 0.14618446\n",
      "Iteration 141, loss = 0.14575609\n",
      "Iteration 142, loss = 0.14532092\n",
      "Iteration 143, loss = 0.14487958\n",
      "Iteration 144, loss = 0.14444891\n",
      "Iteration 145, loss = 0.14402770\n",
      "Iteration 146, loss = 0.14361591\n",
      "Iteration 147, loss = 0.14320745\n",
      "Iteration 148, loss = 0.14280419\n",
      "Iteration 149, loss = 0.14240249\n",
      "Iteration 150, loss = 0.14198678\n",
      "Iteration 151, loss = 0.14160417\n",
      "Iteration 152, loss = 0.14119834\n",
      "Iteration 153, loss = 0.14081109\n",
      "Iteration 154, loss = 0.14043890\n",
      "Iteration 155, loss = 0.14005388\n",
      "Iteration 156, loss = 0.13967956\n",
      "Iteration 157, loss = 0.13930881\n",
      "Iteration 158, loss = 0.13892985\n",
      "Iteration 159, loss = 0.13859195\n",
      "Iteration 160, loss = 0.13821922\n",
      "Iteration 161, loss = 0.13784439\n",
      "Iteration 162, loss = 0.13749925\n",
      "Iteration 163, loss = 0.13715354\n",
      "Iteration 164, loss = 0.13680574\n",
      "Iteration 165, loss = 0.13646503\n",
      "Iteration 166, loss = 0.13613452\n",
      "Iteration 167, loss = 0.13579607\n",
      "Iteration 168, loss = 0.13547358\n",
      "Iteration 169, loss = 0.13513504\n",
      "Iteration 170, loss = 0.13481035\n",
      "Iteration 171, loss = 0.13447320\n",
      "Iteration 172, loss = 0.13415726\n",
      "Iteration 173, loss = 0.13384463\n",
      "Iteration 174, loss = 0.13351125\n",
      "Iteration 175, loss = 0.13320286\n",
      "Iteration 176, loss = 0.13289726\n",
      "Iteration 177, loss = 0.13259941\n",
      "Iteration 178, loss = 0.13228971\n",
      "Iteration 179, loss = 0.13198772\n",
      "Iteration 180, loss = 0.13168136\n",
      "Iteration 181, loss = 0.13138010\n",
      "Iteration 182, loss = 0.13108431\n",
      "Iteration 183, loss = 0.13079709\n",
      "Iteration 184, loss = 0.13050905\n",
      "Iteration 185, loss = 0.13020757\n",
      "Iteration 186, loss = 0.12993462\n",
      "Iteration 187, loss = 0.12964822\n",
      "Iteration 188, loss = 0.12939473\n",
      "Iteration 189, loss = 0.12911542\n",
      "Iteration 190, loss = 0.12884964\n",
      "Iteration 191, loss = 0.12857747\n",
      "Iteration 192, loss = 0.12830391\n",
      "Iteration 193, loss = 0.12803557\n",
      "Iteration 194, loss = 0.12778139\n",
      "Iteration 195, loss = 0.12751493\n",
      "Iteration 196, loss = 0.12725934\n",
      "Iteration 197, loss = 0.12699818\n",
      "Iteration 198, loss = 0.12674111\n",
      "Iteration 199, loss = 0.12649648\n",
      "Iteration 200, loss = 0.12622946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=1000, learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=21, shuffle=True, solver='sgd', tol=1e-09,\n",
       "              validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73255170\n",
      "Iteration 2, loss = 0.71297919\n",
      "Iteration 3, loss = 0.68620563\n",
      "Iteration 4, loss = 0.65816828\n",
      "Iteration 5, loss = 0.63187415\n",
      "Iteration 6, loss = 0.60757385\n",
      "Iteration 7, loss = 0.58516642\n",
      "Iteration 8, loss = 0.56528584\n",
      "Iteration 9, loss = 0.54695118\n",
      "Iteration 10, loss = 0.53006543\n",
      "Iteration 11, loss = 0.51460255\n",
      "Iteration 12, loss = 0.49982957\n",
      "Iteration 13, loss = 0.48623533\n",
      "Iteration 14, loss = 0.47337668\n",
      "Iteration 15, loss = 0.46145027\n",
      "Iteration 16, loss = 0.44998012\n",
      "Iteration 17, loss = 0.43920228\n",
      "Iteration 18, loss = 0.42895455\n",
      "Iteration 19, loss = 0.41936157\n",
      "Iteration 20, loss = 0.41016410\n",
      "Iteration 21, loss = 0.40140491\n",
      "Iteration 22, loss = 0.39311040\n",
      "Iteration 23, loss = 0.38523221\n",
      "Iteration 24, loss = 0.37764809\n",
      "Iteration 25, loss = 0.37037921\n",
      "Iteration 26, loss = 0.36347184\n",
      "Iteration 27, loss = 0.35694542\n",
      "Iteration 28, loss = 0.35060868\n",
      "Iteration 29, loss = 0.34453672\n",
      "Iteration 30, loss = 0.33860465\n",
      "Iteration 31, loss = 0.33310241\n",
      "Iteration 32, loss = 0.32773765\n",
      "Iteration 33, loss = 0.32250277\n",
      "Iteration 34, loss = 0.31758179\n",
      "Iteration 35, loss = 0.31283676\n",
      "Iteration 36, loss = 0.30820868\n",
      "Iteration 37, loss = 0.30377862\n",
      "Iteration 38, loss = 0.29948280\n",
      "Iteration 39, loss = 0.29536767\n",
      "Iteration 40, loss = 0.29137610\n",
      "Iteration 41, loss = 0.28748617\n",
      "Iteration 42, loss = 0.28382520\n",
      "Iteration 43, loss = 0.28021177\n",
      "Iteration 44, loss = 0.27673689\n",
      "Iteration 45, loss = 0.27334082\n",
      "Iteration 46, loss = 0.27016195\n",
      "Iteration 47, loss = 0.26701583\n",
      "Iteration 48, loss = 0.26393392\n",
      "Iteration 49, loss = 0.26097924\n",
      "Iteration 50, loss = 0.25812900\n",
      "Iteration 51, loss = 0.25534158\n",
      "Iteration 52, loss = 0.25265576\n",
      "Iteration 53, loss = 0.25005135\n",
      "Iteration 54, loss = 0.24752279\n",
      "Iteration 55, loss = 0.24506633\n",
      "Iteration 56, loss = 0.24267632\n",
      "Iteration 57, loss = 0.24032383\n",
      "Iteration 58, loss = 0.23804499\n",
      "Iteration 59, loss = 0.23588269\n",
      "Iteration 60, loss = 0.23377505\n",
      "Iteration 61, loss = 0.23169659\n",
      "Iteration 62, loss = 0.22964841\n",
      "Iteration 63, loss = 0.22768983\n",
      "Iteration 64, loss = 0.22575344\n",
      "Iteration 65, loss = 0.22388870\n",
      "Iteration 66, loss = 0.22207076\n",
      "Iteration 67, loss = 0.22031136\n",
      "Iteration 68, loss = 0.21854289\n",
      "Iteration 69, loss = 0.21683176\n",
      "Iteration 70, loss = 0.21520766\n",
      "Iteration 71, loss = 0.21360015\n",
      "Iteration 72, loss = 0.21202536\n",
      "Iteration 73, loss = 0.21048801\n",
      "Iteration 74, loss = 0.20899208\n",
      "Iteration 75, loss = 0.20753362\n",
      "Iteration 76, loss = 0.20611161\n",
      "Iteration 77, loss = 0.20469954\n",
      "Iteration 78, loss = 0.20332985\n",
      "Iteration 79, loss = 0.20199226\n",
      "Iteration 80, loss = 0.20068216\n",
      "Iteration 81, loss = 0.19938886\n",
      "Iteration 82, loss = 0.19812492\n",
      "Iteration 83, loss = 0.19690833\n",
      "Iteration 84, loss = 0.19572703\n",
      "Iteration 85, loss = 0.19451543\n",
      "Iteration 86, loss = 0.19338172\n",
      "Iteration 87, loss = 0.19223184\n",
      "Iteration 88, loss = 0.19114402\n",
      "Iteration 89, loss = 0.19004301\n",
      "Iteration 90, loss = 0.18895900\n",
      "Iteration 91, loss = 0.18791936\n",
      "Iteration 92, loss = 0.18690150\n",
      "Iteration 93, loss = 0.18587352\n",
      "Iteration 94, loss = 0.18490237\n",
      "Iteration 95, loss = 0.18393371\n",
      "Iteration 96, loss = 0.18299045\n",
      "Iteration 97, loss = 0.18202528\n",
      "Iteration 98, loss = 0.18109946\n",
      "Iteration 99, loss = 0.18021022\n",
      "Iteration 100, loss = 0.17934657\n",
      "Iteration 101, loss = 0.17843754\n",
      "Iteration 102, loss = 0.17757097\n",
      "Iteration 103, loss = 0.17673864\n",
      "Iteration 104, loss = 0.17592597\n",
      "Iteration 105, loss = 0.17510721\n",
      "Iteration 106, loss = 0.17427412\n",
      "Iteration 107, loss = 0.17346894\n",
      "Iteration 108, loss = 0.17272071\n",
      "Iteration 109, loss = 0.17194964\n",
      "Iteration 110, loss = 0.17120388\n",
      "Iteration 111, loss = 0.17044382\n",
      "Iteration 112, loss = 0.16973366\n",
      "Iteration 113, loss = 0.16899684\n",
      "Iteration 114, loss = 0.16832487\n",
      "Iteration 115, loss = 0.16763292\n",
      "Iteration 116, loss = 0.16691875\n",
      "Iteration 117, loss = 0.16625206\n",
      "Iteration 118, loss = 0.16557004\n",
      "Iteration 119, loss = 0.16493348\n",
      "Iteration 120, loss = 0.16427910\n",
      "Iteration 121, loss = 0.16366039\n",
      "Iteration 122, loss = 0.16300912\n",
      "Iteration 123, loss = 0.16242274\n",
      "Iteration 124, loss = 0.16177064\n",
      "Iteration 125, loss = 0.16121781\n",
      "Iteration 126, loss = 0.16058359\n",
      "Iteration 127, loss = 0.16002812\n",
      "Iteration 128, loss = 0.15944277\n",
      "Iteration 129, loss = 0.15886433\n",
      "Iteration 130, loss = 0.15832678\n",
      "Iteration 131, loss = 0.15775724\n",
      "Iteration 132, loss = 0.15720365\n",
      "Iteration 133, loss = 0.15667847\n",
      "Iteration 134, loss = 0.15613068\n",
      "Iteration 135, loss = 0.15561523\n",
      "Iteration 136, loss = 0.15508110\n",
      "Iteration 137, loss = 0.15457766\n",
      "Iteration 138, loss = 0.15406437\n",
      "Iteration 139, loss = 0.15355655\n",
      "Iteration 140, loss = 0.15308263\n",
      "Iteration 141, loss = 0.15257990\n",
      "Iteration 142, loss = 0.15210022\n",
      "Iteration 143, loss = 0.15163267\n",
      "Iteration 144, loss = 0.15115736\n",
      "Iteration 145, loss = 0.15069161\n",
      "Iteration 146, loss = 0.15024088\n",
      "Iteration 147, loss = 0.14976842\n",
      "Iteration 148, loss = 0.14933893\n",
      "Iteration 149, loss = 0.14887268\n",
      "Iteration 150, loss = 0.14844194\n",
      "Iteration 151, loss = 0.14800349\n",
      "Iteration 152, loss = 0.14757085\n",
      "Iteration 153, loss = 0.14714147\n",
      "Iteration 154, loss = 0.14671822\n",
      "Iteration 155, loss = 0.14631162\n",
      "Iteration 156, loss = 0.14590345\n",
      "Iteration 157, loss = 0.14548787\n",
      "Iteration 158, loss = 0.14508612\n",
      "Iteration 159, loss = 0.14466552\n",
      "Iteration 160, loss = 0.14428966\n",
      "Iteration 161, loss = 0.14388393\n",
      "Iteration 162, loss = 0.14349045\n",
      "Iteration 163, loss = 0.14312708\n",
      "Iteration 164, loss = 0.14272756\n",
      "Iteration 165, loss = 0.14235582\n",
      "Iteration 166, loss = 0.14198327\n",
      "Iteration 167, loss = 0.14162176\n",
      "Iteration 168, loss = 0.14126205\n",
      "Iteration 169, loss = 0.14089531\n",
      "Iteration 170, loss = 0.14053976\n",
      "Iteration 171, loss = 0.14018707\n",
      "Iteration 172, loss = 0.13982999\n",
      "Iteration 173, loss = 0.13948759\n",
      "Iteration 174, loss = 0.13914292\n",
      "Iteration 175, loss = 0.13880919\n",
      "Iteration 176, loss = 0.13846338\n",
      "Iteration 177, loss = 0.13813371\n",
      "Iteration 178, loss = 0.13779984\n",
      "Iteration 179, loss = 0.13746947\n",
      "Iteration 180, loss = 0.13713823\n",
      "Iteration 181, loss = 0.13682571\n",
      "Iteration 182, loss = 0.13649748\n",
      "Iteration 183, loss = 0.13617695\n",
      "Iteration 184, loss = 0.13587355\n",
      "Iteration 185, loss = 0.13557254\n",
      "Iteration 186, loss = 0.13525563\n",
      "Iteration 187, loss = 0.13494185\n",
      "Iteration 188, loss = 0.13464965\n",
      "Iteration 189, loss = 0.13435011\n",
      "Iteration 190, loss = 0.13405104\n",
      "Iteration 191, loss = 0.13375479\n",
      "Iteration 192, loss = 0.13346086\n",
      "Iteration 193, loss = 0.13317576\n",
      "Iteration 194, loss = 0.13289182\n",
      "Iteration 195, loss = 0.13260391\n",
      "Iteration 196, loss = 0.13231423\n",
      "Iteration 197, loss = 0.13204562\n",
      "Iteration 198, loss = 0.13175684\n",
      "Iteration 199, loss = 0.13149541\n",
      "Iteration 200, loss = 0.13120620\n",
      "Iteration 1, loss = 0.73372226\n",
      "Iteration 2, loss = 0.71353159\n",
      "Iteration 3, loss = 0.68663702\n",
      "Iteration 4, loss = 0.65818403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.63142097\n",
      "Iteration 6, loss = 0.60717235\n",
      "Iteration 7, loss = 0.58435291\n",
      "Iteration 8, loss = 0.56451794\n",
      "Iteration 9, loss = 0.54604857\n",
      "Iteration 10, loss = 0.52918792\n",
      "Iteration 11, loss = 0.51360754\n",
      "Iteration 12, loss = 0.49895993\n",
      "Iteration 13, loss = 0.48540771\n",
      "Iteration 14, loss = 0.47258235\n",
      "Iteration 15, loss = 0.46058013\n",
      "Iteration 16, loss = 0.44919224\n",
      "Iteration 17, loss = 0.43863209\n",
      "Iteration 18, loss = 0.42833880\n",
      "Iteration 19, loss = 0.41879369\n",
      "Iteration 20, loss = 0.40963979\n",
      "Iteration 21, loss = 0.40106907\n",
      "Iteration 22, loss = 0.39276996\n",
      "Iteration 23, loss = 0.38479064\n",
      "Iteration 24, loss = 0.37740107\n",
      "Iteration 25, loss = 0.37023266\n",
      "Iteration 26, loss = 0.36329627\n",
      "Iteration 27, loss = 0.35677591\n",
      "Iteration 28, loss = 0.35058684\n",
      "Iteration 29, loss = 0.34446224\n",
      "Iteration 30, loss = 0.33871539\n",
      "Iteration 31, loss = 0.33318936\n",
      "Iteration 32, loss = 0.32781332\n",
      "Iteration 33, loss = 0.32269229\n",
      "Iteration 34, loss = 0.31778868\n",
      "Iteration 35, loss = 0.31300428\n",
      "Iteration 36, loss = 0.30847784\n",
      "Iteration 37, loss = 0.30398300\n",
      "Iteration 38, loss = 0.29976616\n",
      "Iteration 39, loss = 0.29568607\n",
      "Iteration 40, loss = 0.29173535\n",
      "Iteration 41, loss = 0.28791524\n",
      "Iteration 42, loss = 0.28419552\n",
      "Iteration 43, loss = 0.28069172\n",
      "Iteration 44, loss = 0.27718601\n",
      "Iteration 45, loss = 0.27382707\n",
      "Iteration 46, loss = 0.27068669\n",
      "Iteration 47, loss = 0.26751510\n",
      "Iteration 48, loss = 0.26449485\n",
      "Iteration 49, loss = 0.26157786\n",
      "Iteration 50, loss = 0.25871066\n",
      "Iteration 51, loss = 0.25597160\n",
      "Iteration 52, loss = 0.25330112\n",
      "Iteration 53, loss = 0.25068909\n",
      "Iteration 54, loss = 0.24818067\n",
      "Iteration 55, loss = 0.24572336\n",
      "Iteration 56, loss = 0.24338485\n",
      "Iteration 57, loss = 0.24104743\n",
      "Iteration 58, loss = 0.23879324\n",
      "Iteration 59, loss = 0.23659461\n",
      "Iteration 60, loss = 0.23449978\n",
      "Iteration 61, loss = 0.23241928\n",
      "Iteration 62, loss = 0.23040200\n",
      "Iteration 63, loss = 0.22841903\n",
      "Iteration 64, loss = 0.22650592\n",
      "Iteration 65, loss = 0.22467180\n",
      "Iteration 66, loss = 0.22280573\n",
      "Iteration 67, loss = 0.22103060\n",
      "Iteration 68, loss = 0.21931838\n",
      "Iteration 69, loss = 0.21761078\n",
      "Iteration 70, loss = 0.21596387\n",
      "Iteration 71, loss = 0.21434029\n",
      "Iteration 72, loss = 0.21277077\n",
      "Iteration 73, loss = 0.21124454\n",
      "Iteration 74, loss = 0.20971761\n",
      "Iteration 75, loss = 0.20826897\n",
      "Iteration 76, loss = 0.20682712\n",
      "Iteration 77, loss = 0.20540029\n",
      "Iteration 78, loss = 0.20407024\n",
      "Iteration 79, loss = 0.20268978\n",
      "Iteration 80, loss = 0.20137840\n",
      "Iteration 81, loss = 0.20011336\n",
      "Iteration 82, loss = 0.19880569\n",
      "Iteration 83, loss = 0.19759667\n",
      "Iteration 84, loss = 0.19637427\n",
      "Iteration 85, loss = 0.19520395\n",
      "Iteration 86, loss = 0.19403406\n",
      "Iteration 87, loss = 0.19290294\n",
      "Iteration 88, loss = 0.19178798\n",
      "Iteration 89, loss = 0.19070052\n",
      "Iteration 90, loss = 0.18960573\n",
      "Iteration 91, loss = 0.18856936\n",
      "Iteration 92, loss = 0.18752532\n",
      "Iteration 93, loss = 0.18650774\n",
      "Iteration 94, loss = 0.18553283\n",
      "Iteration 95, loss = 0.18453547\n",
      "Iteration 96, loss = 0.18356979\n",
      "Iteration 97, loss = 0.18262805\n",
      "Iteration 98, loss = 0.18170344\n",
      "Iteration 99, loss = 0.18078522\n",
      "Iteration 100, loss = 0.17990856\n",
      "Iteration 101, loss = 0.17901421\n",
      "Iteration 102, loss = 0.17813111\n",
      "Iteration 103, loss = 0.17729374\n",
      "Iteration 104, loss = 0.17644393\n",
      "Iteration 105, loss = 0.17563148\n",
      "Iteration 106, loss = 0.17482001\n",
      "Iteration 107, loss = 0.17402793\n",
      "Iteration 108, loss = 0.17322457\n",
      "Iteration 109, loss = 0.17245643\n",
      "Iteration 110, loss = 0.17170293\n",
      "Iteration 111, loss = 0.17095150\n",
      "Iteration 112, loss = 0.17020567\n",
      "Iteration 113, loss = 0.16950337\n",
      "Iteration 114, loss = 0.16877494\n",
      "Iteration 115, loss = 0.16805679\n",
      "Iteration 116, loss = 0.16736789\n",
      "Iteration 117, loss = 0.16668195\n",
      "Iteration 118, loss = 0.16602092\n",
      "Iteration 119, loss = 0.16534989\n",
      "Iteration 120, loss = 0.16469506\n",
      "Iteration 121, loss = 0.16403522\n",
      "Iteration 122, loss = 0.16339849\n",
      "Iteration 123, loss = 0.16279889\n",
      "Iteration 124, loss = 0.16214670\n",
      "Iteration 125, loss = 0.16157344\n",
      "Iteration 126, loss = 0.16094677\n",
      "Iteration 127, loss = 0.16034647\n",
      "Iteration 128, loss = 0.15975370\n",
      "Iteration 129, loss = 0.15919767\n",
      "Iteration 130, loss = 0.15861930\n",
      "Iteration 131, loss = 0.15807022\n",
      "Iteration 132, loss = 0.15748145\n",
      "Iteration 133, loss = 0.15695815\n",
      "Iteration 134, loss = 0.15639464\n",
      "Iteration 135, loss = 0.15587404\n",
      "Iteration 136, loss = 0.15533430\n",
      "Iteration 137, loss = 0.15482007\n",
      "Iteration 138, loss = 0.15430743\n",
      "Iteration 139, loss = 0.15379222\n",
      "Iteration 140, loss = 0.15329134\n",
      "Iteration 141, loss = 0.15278715\n",
      "Iteration 142, loss = 0.15229542\n",
      "Iteration 143, loss = 0.15183686\n",
      "Iteration 144, loss = 0.15132531\n",
      "Iteration 145, loss = 0.15085548\n",
      "Iteration 146, loss = 0.15039196\n",
      "Iteration 147, loss = 0.14991859\n",
      "Iteration 148, loss = 0.14948301\n",
      "Iteration 149, loss = 0.14901310\n",
      "Iteration 150, loss = 0.14855510\n",
      "Iteration 151, loss = 0.14813805\n",
      "Iteration 152, loss = 0.14769696\n",
      "Iteration 153, loss = 0.14724641\n",
      "Iteration 154, loss = 0.14680874\n",
      "Iteration 155, loss = 0.14639710\n",
      "Iteration 156, loss = 0.14597465\n",
      "Iteration 157, loss = 0.14556060\n",
      "Iteration 158, loss = 0.14516350\n",
      "Iteration 159, loss = 0.14475127\n",
      "Iteration 160, loss = 0.14434117\n",
      "Iteration 161, loss = 0.14394911\n",
      "Iteration 162, loss = 0.14355390\n",
      "Iteration 163, loss = 0.14316056\n",
      "Iteration 164, loss = 0.14278440\n",
      "Iteration 165, loss = 0.14240328\n",
      "Iteration 166, loss = 0.14202235\n",
      "Iteration 167, loss = 0.14166475\n",
      "Iteration 168, loss = 0.14129107\n",
      "Iteration 169, loss = 0.14090173\n",
      "Iteration 170, loss = 0.14055683\n",
      "Iteration 171, loss = 0.14020819\n",
      "Iteration 172, loss = 0.13984532\n",
      "Iteration 173, loss = 0.13948487\n",
      "Iteration 174, loss = 0.13914519\n",
      "Iteration 175, loss = 0.13878814\n",
      "Iteration 176, loss = 0.13845443\n",
      "Iteration 177, loss = 0.13811552\n",
      "Iteration 178, loss = 0.13776655\n",
      "Iteration 179, loss = 0.13744386\n",
      "Iteration 180, loss = 0.13711325\n",
      "Iteration 181, loss = 0.13678745\n",
      "Iteration 182, loss = 0.13646103\n",
      "Iteration 183, loss = 0.13614102\n",
      "Iteration 184, loss = 0.13582076\n",
      "Iteration 185, loss = 0.13550988\n",
      "Iteration 186, loss = 0.13519656\n",
      "Iteration 187, loss = 0.13488660\n",
      "Iteration 188, loss = 0.13457677\n",
      "Iteration 189, loss = 0.13428572\n",
      "Iteration 190, loss = 0.13398473\n",
      "Iteration 191, loss = 0.13367410\n",
      "Iteration 192, loss = 0.13338183\n",
      "Iteration 193, loss = 0.13309387\n",
      "Iteration 194, loss = 0.13279351\n",
      "Iteration 195, loss = 0.13250263\n",
      "Iteration 196, loss = 0.13223089\n",
      "Iteration 197, loss = 0.13193663\n",
      "Iteration 198, loss = 0.13164235\n",
      "Iteration 199, loss = 0.13137052\n",
      "Iteration 200, loss = 0.13109230\n",
      "Iteration 1, loss = 0.73624503\n",
      "Iteration 2, loss = 0.71552720\n",
      "Iteration 3, loss = 0.68812163\n",
      "Iteration 4, loss = 0.65900100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.63175228\n",
      "Iteration 6, loss = 0.60742741\n",
      "Iteration 7, loss = 0.58413658\n",
      "Iteration 8, loss = 0.56420703\n",
      "Iteration 9, loss = 0.54573083\n",
      "Iteration 10, loss = 0.52868403\n",
      "Iteration 11, loss = 0.51312251\n",
      "Iteration 12, loss = 0.49841855\n",
      "Iteration 13, loss = 0.48478226\n",
      "Iteration 14, loss = 0.47193446\n",
      "Iteration 15, loss = 0.45989754\n",
      "Iteration 16, loss = 0.44845625\n",
      "Iteration 17, loss = 0.43782423\n",
      "Iteration 18, loss = 0.42749598\n",
      "Iteration 19, loss = 0.41788625\n",
      "Iteration 20, loss = 0.40870193\n",
      "Iteration 21, loss = 0.40004516\n",
      "Iteration 22, loss = 0.39172544\n",
      "Iteration 23, loss = 0.38371990\n",
      "Iteration 24, loss = 0.37626861\n",
      "Iteration 25, loss = 0.36907138\n",
      "Iteration 26, loss = 0.36208030\n",
      "Iteration 27, loss = 0.35555032\n",
      "Iteration 28, loss = 0.34926155\n",
      "Iteration 29, loss = 0.34317432\n",
      "Iteration 30, loss = 0.33734103\n",
      "Iteration 31, loss = 0.33175752\n",
      "Iteration 32, loss = 0.32634644\n",
      "Iteration 33, loss = 0.32121185\n",
      "Iteration 34, loss = 0.31623996\n",
      "Iteration 35, loss = 0.31146741\n",
      "Iteration 36, loss = 0.30688810\n",
      "Iteration 37, loss = 0.30234302\n",
      "Iteration 38, loss = 0.29811799\n",
      "Iteration 39, loss = 0.29401772\n",
      "Iteration 40, loss = 0.29005100\n",
      "Iteration 41, loss = 0.28616597\n",
      "Iteration 42, loss = 0.28242617\n",
      "Iteration 43, loss = 0.27893396\n",
      "Iteration 44, loss = 0.27536346\n",
      "Iteration 45, loss = 0.27198761\n",
      "Iteration 46, loss = 0.26881634\n",
      "Iteration 47, loss = 0.26562179\n",
      "Iteration 48, loss = 0.26259393\n",
      "Iteration 49, loss = 0.25963826\n",
      "Iteration 50, loss = 0.25676119\n",
      "Iteration 51, loss = 0.25398780\n",
      "Iteration 52, loss = 0.25130055\n",
      "Iteration 53, loss = 0.24867751\n",
      "Iteration 54, loss = 0.24616495\n",
      "Iteration 55, loss = 0.24365846\n",
      "Iteration 56, loss = 0.24130799\n",
      "Iteration 57, loss = 0.23895568\n",
      "Iteration 58, loss = 0.23668143\n",
      "Iteration 59, loss = 0.23448454\n",
      "Iteration 60, loss = 0.23233769\n",
      "Iteration 61, loss = 0.23025430\n",
      "Iteration 62, loss = 0.22821310\n",
      "Iteration 63, loss = 0.22620789\n",
      "Iteration 64, loss = 0.22428625\n",
      "Iteration 65, loss = 0.22243744\n",
      "Iteration 66, loss = 0.22056464\n",
      "Iteration 67, loss = 0.21876754\n",
      "Iteration 68, loss = 0.21704829\n",
      "Iteration 69, loss = 0.21532854\n",
      "Iteration 70, loss = 0.21367200\n",
      "Iteration 71, loss = 0.21203181\n",
      "Iteration 72, loss = 0.21046708\n",
      "Iteration 73, loss = 0.20892947\n",
      "Iteration 74, loss = 0.20738472\n",
      "Iteration 75, loss = 0.20593436\n",
      "Iteration 76, loss = 0.20447991\n",
      "Iteration 77, loss = 0.20304328\n",
      "Iteration 78, loss = 0.20170905\n",
      "Iteration 79, loss = 0.20033331\n",
      "Iteration 80, loss = 0.19901051\n",
      "Iteration 81, loss = 0.19772033\n",
      "Iteration 82, loss = 0.19643393\n",
      "Iteration 83, loss = 0.19521348\n",
      "Iteration 84, loss = 0.19396631\n",
      "Iteration 85, loss = 0.19280714\n",
      "Iteration 86, loss = 0.19163712\n",
      "Iteration 87, loss = 0.19049004\n",
      "Iteration 88, loss = 0.18936795\n",
      "Iteration 89, loss = 0.18827779\n",
      "Iteration 90, loss = 0.18717412\n",
      "Iteration 91, loss = 0.18613162\n",
      "Iteration 92, loss = 0.18508701\n",
      "Iteration 93, loss = 0.18406882\n",
      "Iteration 94, loss = 0.18309790\n",
      "Iteration 95, loss = 0.18208823\n",
      "Iteration 96, loss = 0.18112608\n",
      "Iteration 97, loss = 0.18018255\n",
      "Iteration 98, loss = 0.17925917\n",
      "Iteration 99, loss = 0.17830641\n",
      "Iteration 100, loss = 0.17745306\n",
      "Iteration 101, loss = 0.17655363\n",
      "Iteration 102, loss = 0.17567229\n",
      "Iteration 103, loss = 0.17482168\n",
      "Iteration 104, loss = 0.17397969\n",
      "Iteration 105, loss = 0.17315343\n",
      "Iteration 106, loss = 0.17235644\n",
      "Iteration 107, loss = 0.17155051\n",
      "Iteration 108, loss = 0.17074250\n",
      "Iteration 109, loss = 0.16998331\n",
      "Iteration 110, loss = 0.16922898\n",
      "Iteration 111, loss = 0.16845990\n",
      "Iteration 112, loss = 0.16772770\n",
      "Iteration 113, loss = 0.16702090\n",
      "Iteration 114, loss = 0.16628873\n",
      "Iteration 115, loss = 0.16558997\n",
      "Iteration 116, loss = 0.16488293\n",
      "Iteration 117, loss = 0.16418282\n",
      "Iteration 118, loss = 0.16353517\n",
      "Iteration 119, loss = 0.16286055\n",
      "Iteration 120, loss = 0.16220030\n",
      "Iteration 121, loss = 0.16154552\n",
      "Iteration 122, loss = 0.16090568\n",
      "Iteration 123, loss = 0.16029923\n",
      "Iteration 124, loss = 0.15964759\n",
      "Iteration 125, loss = 0.15906803\n",
      "Iteration 126, loss = 0.15844205\n",
      "Iteration 127, loss = 0.15783611\n",
      "Iteration 128, loss = 0.15724863\n",
      "Iteration 129, loss = 0.15668523\n",
      "Iteration 130, loss = 0.15610572\n",
      "Iteration 131, loss = 0.15556928\n",
      "Iteration 132, loss = 0.15496510\n",
      "Iteration 133, loss = 0.15444829\n",
      "Iteration 134, loss = 0.15388185\n",
      "Iteration 135, loss = 0.15336357\n",
      "Iteration 136, loss = 0.15281826\n",
      "Iteration 137, loss = 0.15231108\n",
      "Iteration 138, loss = 0.15178921\n",
      "Iteration 139, loss = 0.15127830\n",
      "Iteration 140, loss = 0.15077501\n",
      "Iteration 141, loss = 0.15027649\n",
      "Iteration 142, loss = 0.14978246\n",
      "Iteration 143, loss = 0.14931815\n",
      "Iteration 144, loss = 0.14880984\n",
      "Iteration 145, loss = 0.14833996\n",
      "Iteration 146, loss = 0.14788065\n",
      "Iteration 147, loss = 0.14740288\n",
      "Iteration 148, loss = 0.14695745\n",
      "Iteration 149, loss = 0.14649240\n",
      "Iteration 150, loss = 0.14603357\n",
      "Iteration 151, loss = 0.14561236\n",
      "Iteration 152, loss = 0.14517330\n",
      "Iteration 153, loss = 0.14472172\n",
      "Iteration 154, loss = 0.14429043\n",
      "Iteration 155, loss = 0.14386563\n",
      "Iteration 156, loss = 0.14344907\n",
      "Iteration 157, loss = 0.14302478\n",
      "Iteration 158, loss = 0.14263306\n",
      "Iteration 159, loss = 0.14221329\n",
      "Iteration 160, loss = 0.14181816\n",
      "Iteration 161, loss = 0.14141451\n",
      "Iteration 162, loss = 0.14101388\n",
      "Iteration 163, loss = 0.14062434\n",
      "Iteration 164, loss = 0.14024296\n",
      "Iteration 165, loss = 0.13985817\n",
      "Iteration 166, loss = 0.13947898\n",
      "Iteration 167, loss = 0.13911117\n",
      "Iteration 168, loss = 0.13874363\n",
      "Iteration 169, loss = 0.13835309\n",
      "Iteration 170, loss = 0.13799823\n",
      "Iteration 171, loss = 0.13765089\n",
      "Iteration 172, loss = 0.13727868\n",
      "Iteration 173, loss = 0.13692379\n",
      "Iteration 174, loss = 0.13657791\n",
      "Iteration 175, loss = 0.13622755\n",
      "Iteration 176, loss = 0.13587796\n",
      "Iteration 177, loss = 0.13553630\n",
      "Iteration 178, loss = 0.13519453\n",
      "Iteration 179, loss = 0.13486310\n",
      "Iteration 180, loss = 0.13452739\n",
      "Iteration 181, loss = 0.13420030\n",
      "Iteration 182, loss = 0.13387250\n",
      "Iteration 183, loss = 0.13354627\n",
      "Iteration 184, loss = 0.13322721\n",
      "Iteration 185, loss = 0.13290634\n",
      "Iteration 186, loss = 0.13259215\n",
      "Iteration 187, loss = 0.13227616\n",
      "Iteration 188, loss = 0.13196535\n",
      "Iteration 189, loss = 0.13166920\n",
      "Iteration 190, loss = 0.13136857\n",
      "Iteration 191, loss = 0.13105346\n",
      "Iteration 192, loss = 0.13075546\n",
      "Iteration 193, loss = 0.13046166\n",
      "Iteration 194, loss = 0.13015826\n",
      "Iteration 195, loss = 0.12986357\n",
      "Iteration 196, loss = 0.12958458\n",
      "Iteration 197, loss = 0.12928921\n",
      "Iteration 198, loss = 0.12899230\n",
      "Iteration 199, loss = 0.12871583\n",
      "Iteration 200, loss = 0.12842964\n",
      "Iteration 1, loss = 0.73283053\n",
      "Iteration 2, loss = 0.71215421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.68423323\n",
      "Iteration 4, loss = 0.65542844\n",
      "Iteration 5, loss = 0.62802947\n",
      "Iteration 6, loss = 0.60300027\n",
      "Iteration 7, loss = 0.58058488\n",
      "Iteration 8, loss = 0.56022171\n",
      "Iteration 9, loss = 0.54171793\n",
      "Iteration 10, loss = 0.52441915\n",
      "Iteration 11, loss = 0.50908045\n",
      "Iteration 12, loss = 0.49449905\n",
      "Iteration 13, loss = 0.48073105\n",
      "Iteration 14, loss = 0.46804772\n",
      "Iteration 15, loss = 0.45604179\n",
      "Iteration 16, loss = 0.44488238\n",
      "Iteration 17, loss = 0.43427374\n",
      "Iteration 18, loss = 0.42408277\n",
      "Iteration 19, loss = 0.41453929\n",
      "Iteration 20, loss = 0.40538824\n",
      "Iteration 21, loss = 0.39691094\n",
      "Iteration 22, loss = 0.38870765\n",
      "Iteration 23, loss = 0.38089261\n",
      "Iteration 24, loss = 0.37345324\n",
      "Iteration 25, loss = 0.36629519\n",
      "Iteration 26, loss = 0.35952296\n",
      "Iteration 27, loss = 0.35300435\n",
      "Iteration 28, loss = 0.34684565\n",
      "Iteration 29, loss = 0.34087196\n",
      "Iteration 30, loss = 0.33514852\n",
      "Iteration 31, loss = 0.32970011\n",
      "Iteration 32, loss = 0.32446688\n",
      "Iteration 33, loss = 0.31937138\n",
      "Iteration 34, loss = 0.31457649\n",
      "Iteration 35, loss = 0.30989975\n",
      "Iteration 36, loss = 0.30547216\n",
      "Iteration 37, loss = 0.30115759\n",
      "Iteration 38, loss = 0.29694342\n",
      "Iteration 39, loss = 0.29292824\n",
      "Iteration 40, loss = 0.28908686\n",
      "Iteration 41, loss = 0.28532057\n",
      "Iteration 42, loss = 0.28174522\n",
      "Iteration 43, loss = 0.27825266\n",
      "Iteration 44, loss = 0.27489946\n",
      "Iteration 45, loss = 0.27163317\n",
      "Iteration 46, loss = 0.26851350\n",
      "Iteration 47, loss = 0.26545536\n",
      "Iteration 48, loss = 0.26248243\n",
      "Iteration 49, loss = 0.25967025\n",
      "Iteration 50, loss = 0.25689117\n",
      "Iteration 51, loss = 0.25423055\n",
      "Iteration 52, loss = 0.25159668\n",
      "Iteration 53, loss = 0.24907211\n",
      "Iteration 54, loss = 0.24665621\n",
      "Iteration 55, loss = 0.24427015\n",
      "Iteration 56, loss = 0.24194573\n",
      "Iteration 57, loss = 0.23974102\n",
      "Iteration 58, loss = 0.23752210\n",
      "Iteration 59, loss = 0.23543444\n",
      "Iteration 60, loss = 0.23337650\n",
      "Iteration 61, loss = 0.23135883\n",
      "Iteration 62, loss = 0.22940864\n",
      "Iteration 63, loss = 0.22750553\n",
      "Iteration 64, loss = 0.22566961\n",
      "Iteration 65, loss = 0.22388888\n",
      "Iteration 66, loss = 0.22212785\n",
      "Iteration 67, loss = 0.22040332\n",
      "Iteration 68, loss = 0.21878557\n",
      "Iteration 69, loss = 0.21709371\n",
      "Iteration 70, loss = 0.21552670\n",
      "Iteration 71, loss = 0.21402991\n",
      "Iteration 72, loss = 0.21247593\n",
      "Iteration 73, loss = 0.21100003\n",
      "Iteration 74, loss = 0.20953362\n",
      "Iteration 75, loss = 0.20816221\n",
      "Iteration 76, loss = 0.20678916\n",
      "Iteration 77, loss = 0.20542106\n",
      "Iteration 78, loss = 0.20409828\n",
      "Iteration 79, loss = 0.20283575\n",
      "Iteration 80, loss = 0.20156337\n",
      "Iteration 81, loss = 0.20033692\n",
      "Iteration 82, loss = 0.19911906\n",
      "Iteration 83, loss = 0.19795713\n",
      "Iteration 84, loss = 0.19678480\n",
      "Iteration 85, loss = 0.19565566\n",
      "Iteration 86, loss = 0.19454345\n",
      "Iteration 87, loss = 0.19346803\n",
      "Iteration 88, loss = 0.19239765\n",
      "Iteration 89, loss = 0.19134600\n",
      "Iteration 90, loss = 0.19033178\n",
      "Iteration 91, loss = 0.18930735\n",
      "Iteration 92, loss = 0.18831582\n",
      "Iteration 93, loss = 0.18735480\n",
      "Iteration 94, loss = 0.18641011\n",
      "Iteration 95, loss = 0.18549770\n",
      "Iteration 96, loss = 0.18452695\n",
      "Iteration 97, loss = 0.18364110\n",
      "Iteration 98, loss = 0.18274688\n",
      "Iteration 99, loss = 0.18188345\n",
      "Iteration 100, loss = 0.18102486\n",
      "Iteration 101, loss = 0.18018332\n",
      "Iteration 102, loss = 0.17936282\n",
      "Iteration 103, loss = 0.17853489\n",
      "Iteration 104, loss = 0.17774901\n",
      "Iteration 105, loss = 0.17696613\n",
      "Iteration 106, loss = 0.17617243\n",
      "Iteration 107, loss = 0.17541925\n",
      "Iteration 108, loss = 0.17467675\n",
      "Iteration 109, loss = 0.17393050\n",
      "Iteration 110, loss = 0.17320026\n",
      "Iteration 111, loss = 0.17250116\n",
      "Iteration 112, loss = 0.17180763\n",
      "Iteration 113, loss = 0.17108616\n",
      "Iteration 114, loss = 0.17041034\n",
      "Iteration 115, loss = 0.16971965\n",
      "Iteration 116, loss = 0.16906684\n",
      "Iteration 117, loss = 0.16841039\n",
      "Iteration 118, loss = 0.16777612\n",
      "Iteration 119, loss = 0.16712770\n",
      "Iteration 120, loss = 0.16651067\n",
      "Iteration 121, loss = 0.16591745\n",
      "Iteration 122, loss = 0.16527028\n",
      "Iteration 123, loss = 0.16467495\n",
      "Iteration 124, loss = 0.16407616\n",
      "Iteration 125, loss = 0.16348953\n",
      "Iteration 126, loss = 0.16292904\n",
      "Iteration 127, loss = 0.16234425\n",
      "Iteration 128, loss = 0.16178314\n",
      "Iteration 129, loss = 0.16121342\n",
      "Iteration 130, loss = 0.16065918\n",
      "Iteration 131, loss = 0.16014129\n",
      "Iteration 132, loss = 0.15959246\n",
      "Iteration 133, loss = 0.15905534\n",
      "Iteration 134, loss = 0.15854789\n",
      "Iteration 135, loss = 0.15802469\n",
      "Iteration 136, loss = 0.15754159\n",
      "Iteration 137, loss = 0.15701463\n",
      "Iteration 138, loss = 0.15652375\n",
      "Iteration 139, loss = 0.15603264\n",
      "Iteration 140, loss = 0.15556326\n",
      "Iteration 141, loss = 0.15506187\n",
      "Iteration 142, loss = 0.15459761\n",
      "Iteration 143, loss = 0.15413874\n",
      "Iteration 144, loss = 0.15365749\n",
      "Iteration 145, loss = 0.15321270\n",
      "Iteration 146, loss = 0.15274788\n",
      "Iteration 147, loss = 0.15230609\n",
      "Iteration 148, loss = 0.15185579\n",
      "Iteration 149, loss = 0.15141360\n",
      "Iteration 150, loss = 0.15099735\n",
      "Iteration 151, loss = 0.15054682\n",
      "Iteration 152, loss = 0.15012300\n",
      "Iteration 153, loss = 0.14971204\n",
      "Iteration 154, loss = 0.14928913\n",
      "Iteration 155, loss = 0.14886992\n",
      "Iteration 156, loss = 0.14845524\n",
      "Iteration 157, loss = 0.14805012\n",
      "Iteration 158, loss = 0.14765949\n",
      "Iteration 159, loss = 0.14725988\n",
      "Iteration 160, loss = 0.14687939\n",
      "Iteration 161, loss = 0.14647349\n",
      "Iteration 162, loss = 0.14609854\n",
      "Iteration 163, loss = 0.14571387\n",
      "Iteration 164, loss = 0.14533106\n",
      "Iteration 165, loss = 0.14496278\n",
      "Iteration 166, loss = 0.14459573\n",
      "Iteration 167, loss = 0.14422074\n",
      "Iteration 168, loss = 0.14386131\n",
      "Iteration 169, loss = 0.14350973\n",
      "Iteration 170, loss = 0.14315728\n",
      "Iteration 171, loss = 0.14279362\n",
      "Iteration 172, loss = 0.14244868\n",
      "Iteration 173, loss = 0.14210803\n",
      "Iteration 174, loss = 0.14176790\n",
      "Iteration 175, loss = 0.14141452\n",
      "Iteration 176, loss = 0.14110357\n",
      "Iteration 177, loss = 0.14076730\n",
      "Iteration 178, loss = 0.14042319\n",
      "Iteration 179, loss = 0.14011778\n",
      "Iteration 180, loss = 0.13976845\n",
      "Iteration 181, loss = 0.13944558\n",
      "Iteration 182, loss = 0.13914512\n",
      "Iteration 183, loss = 0.13880674\n",
      "Iteration 184, loss = 0.13850040\n",
      "Iteration 185, loss = 0.13819460\n",
      "Iteration 186, loss = 0.13789464\n",
      "Iteration 187, loss = 0.13757787\n",
      "Iteration 188, loss = 0.13729151\n",
      "Iteration 189, loss = 0.13697483\n",
      "Iteration 190, loss = 0.13668009\n",
      "Iteration 191, loss = 0.13638337\n",
      "Iteration 192, loss = 0.13610088\n",
      "Iteration 193, loss = 0.13580695\n",
      "Iteration 194, loss = 0.13551026\n",
      "Iteration 195, loss = 0.13522991\n",
      "Iteration 196, loss = 0.13494766\n",
      "Iteration 197, loss = 0.13466972\n",
      "Iteration 198, loss = 0.13438572\n",
      "Iteration 199, loss = 0.13410917\n",
      "Iteration 200, loss = 0.13383922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73293757\n",
      "Iteration 2, loss = 0.71289401\n",
      "Iteration 3, loss = 0.68620644\n",
      "Iteration 4, loss = 0.65803106\n",
      "Iteration 5, loss = 0.63139624\n",
      "Iteration 6, loss = 0.60671725\n",
      "Iteration 7, loss = 0.58463970\n",
      "Iteration 8, loss = 0.56466513\n",
      "Iteration 9, loss = 0.54624352\n",
      "Iteration 10, loss = 0.52908578\n",
      "Iteration 11, loss = 0.51363302\n",
      "Iteration 12, loss = 0.49898298\n",
      "Iteration 13, loss = 0.48526749\n",
      "Iteration 14, loss = 0.47249098\n",
      "Iteration 15, loss = 0.46040217\n",
      "Iteration 16, loss = 0.44904683\n",
      "Iteration 17, loss = 0.43830644\n",
      "Iteration 18, loss = 0.42805826\n",
      "Iteration 19, loss = 0.41838465\n",
      "Iteration 20, loss = 0.40905709\n",
      "Iteration 21, loss = 0.40041233\n",
      "Iteration 22, loss = 0.39211006\n",
      "Iteration 23, loss = 0.38412491\n",
      "Iteration 24, loss = 0.37659571\n",
      "Iteration 25, loss = 0.36924186\n",
      "Iteration 26, loss = 0.36232117\n",
      "Iteration 27, loss = 0.35568757\n",
      "Iteration 28, loss = 0.34932242\n",
      "Iteration 29, loss = 0.34326334\n",
      "Iteration 30, loss = 0.33737325\n",
      "Iteration 31, loss = 0.33177176\n",
      "Iteration 32, loss = 0.32641949\n",
      "Iteration 33, loss = 0.32115685\n",
      "Iteration 34, loss = 0.31620657\n",
      "Iteration 35, loss = 0.31140910\n",
      "Iteration 36, loss = 0.30681395\n",
      "Iteration 37, loss = 0.30234439\n",
      "Iteration 38, loss = 0.29801655\n",
      "Iteration 39, loss = 0.29386367\n",
      "Iteration 40, loss = 0.28987634\n",
      "Iteration 41, loss = 0.28599708\n",
      "Iteration 42, loss = 0.28226642\n",
      "Iteration 43, loss = 0.27864659\n",
      "Iteration 44, loss = 0.27518222\n",
      "Iteration 45, loss = 0.27176446\n",
      "Iteration 46, loss = 0.26852693\n",
      "Iteration 47, loss = 0.26534133\n",
      "Iteration 48, loss = 0.26226069\n",
      "Iteration 49, loss = 0.25933714\n",
      "Iteration 50, loss = 0.25641246\n",
      "Iteration 51, loss = 0.25359855\n",
      "Iteration 52, loss = 0.25090321\n",
      "Iteration 53, loss = 0.24827627\n",
      "Iteration 54, loss = 0.24570609\n",
      "Iteration 55, loss = 0.24322012\n",
      "Iteration 56, loss = 0.24081090\n",
      "Iteration 57, loss = 0.23847813\n",
      "Iteration 58, loss = 0.23617362\n",
      "Iteration 59, loss = 0.23398926\n",
      "Iteration 60, loss = 0.23182537\n",
      "Iteration 61, loss = 0.22970026\n",
      "Iteration 62, loss = 0.22764682\n",
      "Iteration 63, loss = 0.22568525\n",
      "Iteration 64, loss = 0.22372984\n",
      "Iteration 65, loss = 0.22186498\n",
      "Iteration 66, loss = 0.22000681\n",
      "Iteration 67, loss = 0.21820412\n",
      "Iteration 68, loss = 0.21647178\n",
      "Iteration 69, loss = 0.21470218\n",
      "Iteration 70, loss = 0.21304433\n",
      "Iteration 71, loss = 0.21146924\n",
      "Iteration 72, loss = 0.20985107\n",
      "Iteration 73, loss = 0.20827291\n",
      "Iteration 74, loss = 0.20673857\n",
      "Iteration 75, loss = 0.20526675\n",
      "Iteration 76, loss = 0.20384017\n",
      "Iteration 77, loss = 0.20238364\n",
      "Iteration 78, loss = 0.20098820\n",
      "Iteration 79, loss = 0.19965048\n",
      "Iteration 80, loss = 0.19831066\n",
      "Iteration 81, loss = 0.19701494\n",
      "Iteration 82, loss = 0.19573135\n",
      "Iteration 83, loss = 0.19448953\n",
      "Iteration 84, loss = 0.19326432\n",
      "Iteration 85, loss = 0.19208236\n",
      "Iteration 86, loss = 0.19089371\n",
      "Iteration 87, loss = 0.18975814\n",
      "Iteration 88, loss = 0.18861873\n",
      "Iteration 89, loss = 0.18751244\n",
      "Iteration 90, loss = 0.18642647\n",
      "Iteration 91, loss = 0.18536712\n",
      "Iteration 92, loss = 0.18429228\n",
      "Iteration 93, loss = 0.18328973\n",
      "Iteration 94, loss = 0.18228212\n",
      "Iteration 95, loss = 0.18131752\n",
      "Iteration 96, loss = 0.18029650\n",
      "Iteration 97, loss = 0.17934976\n",
      "Iteration 98, loss = 0.17841708\n",
      "Iteration 99, loss = 0.17748830\n",
      "Iteration 100, loss = 0.17659069\n",
      "Iteration 101, loss = 0.17569844\n",
      "Iteration 102, loss = 0.17483303\n",
      "Iteration 103, loss = 0.17397372\n",
      "Iteration 104, loss = 0.17313488\n",
      "Iteration 105, loss = 0.17230544\n",
      "Iteration 106, loss = 0.17147633\n",
      "Iteration 107, loss = 0.17068913\n",
      "Iteration 108, loss = 0.16989639\n",
      "Iteration 109, loss = 0.16911377\n",
      "Iteration 110, loss = 0.16835524\n",
      "Iteration 111, loss = 0.16761114\n",
      "Iteration 112, loss = 0.16688346\n",
      "Iteration 113, loss = 0.16613917\n",
      "Iteration 114, loss = 0.16543179\n",
      "Iteration 115, loss = 0.16470540\n",
      "Iteration 116, loss = 0.16401335\n",
      "Iteration 117, loss = 0.16334557\n",
      "Iteration 118, loss = 0.16267057\n",
      "Iteration 119, loss = 0.16199116\n",
      "Iteration 120, loss = 0.16135689\n",
      "Iteration 121, loss = 0.16072154\n",
      "Iteration 122, loss = 0.16005938\n",
      "Iteration 123, loss = 0.15943755\n",
      "Iteration 124, loss = 0.15880186\n",
      "Iteration 125, loss = 0.15818596\n",
      "Iteration 126, loss = 0.15760398\n",
      "Iteration 127, loss = 0.15700155\n",
      "Iteration 128, loss = 0.15642344\n",
      "Iteration 129, loss = 0.15582446\n",
      "Iteration 130, loss = 0.15525737\n",
      "Iteration 131, loss = 0.15470865\n",
      "Iteration 132, loss = 0.15414235\n",
      "Iteration 133, loss = 0.15359276\n",
      "Iteration 134, loss = 0.15306217\n",
      "Iteration 135, loss = 0.15252106\n",
      "Iteration 136, loss = 0.15201272\n",
      "Iteration 137, loss = 0.15148461\n",
      "Iteration 138, loss = 0.15098033\n",
      "Iteration 139, loss = 0.15046097\n",
      "Iteration 140, loss = 0.14997277\n",
      "Iteration 141, loss = 0.14947324\n",
      "Iteration 142, loss = 0.14899322\n",
      "Iteration 143, loss = 0.14852025\n",
      "Iteration 144, loss = 0.14802461\n",
      "Iteration 145, loss = 0.14756839\n",
      "Iteration 146, loss = 0.14709295\n",
      "Iteration 147, loss = 0.14664357\n",
      "Iteration 148, loss = 0.14618324\n",
      "Iteration 149, loss = 0.14573223\n",
      "Iteration 150, loss = 0.14529990\n",
      "Iteration 151, loss = 0.14485690\n",
      "Iteration 152, loss = 0.14442100\n",
      "Iteration 153, loss = 0.14399540\n",
      "Iteration 154, loss = 0.14357048\n",
      "Iteration 155, loss = 0.14313933\n",
      "Iteration 156, loss = 0.14273006\n",
      "Iteration 157, loss = 0.14231013\n",
      "Iteration 158, loss = 0.14191246\n",
      "Iteration 159, loss = 0.14151025\n",
      "Iteration 160, loss = 0.14112017\n",
      "Iteration 161, loss = 0.14071440\n",
      "Iteration 162, loss = 0.14033660\n",
      "Iteration 163, loss = 0.13994200\n",
      "Iteration 164, loss = 0.13955921\n",
      "Iteration 165, loss = 0.13919082\n",
      "Iteration 166, loss = 0.13881580\n",
      "Iteration 167, loss = 0.13843840\n",
      "Iteration 168, loss = 0.13808418\n",
      "Iteration 169, loss = 0.13772451\n",
      "Iteration 170, loss = 0.13737180\n",
      "Iteration 171, loss = 0.13700857\n",
      "Iteration 172, loss = 0.13666008\n",
      "Iteration 173, loss = 0.13631593\n",
      "Iteration 174, loss = 0.13597919\n",
      "Iteration 175, loss = 0.13562589\n",
      "Iteration 176, loss = 0.13530905\n",
      "Iteration 177, loss = 0.13496571\n",
      "Iteration 178, loss = 0.13463339\n",
      "Iteration 179, loss = 0.13433292\n",
      "Iteration 180, loss = 0.13398094\n",
      "Iteration 181, loss = 0.13366920\n",
      "Iteration 182, loss = 0.13335627\n",
      "Iteration 183, loss = 0.13303191\n",
      "Iteration 184, loss = 0.13273137\n",
      "Iteration 185, loss = 0.13242839\n",
      "Iteration 186, loss = 0.13212262\n",
      "Iteration 187, loss = 0.13181513\n",
      "Iteration 188, loss = 0.13152292\n",
      "Iteration 189, loss = 0.13122151\n",
      "Iteration 190, loss = 0.13092482\n",
      "Iteration 191, loss = 0.13063619\n",
      "Iteration 192, loss = 0.13035243\n",
      "Iteration 193, loss = 0.13006406\n",
      "Iteration 194, loss = 0.12977359\n",
      "Iteration 195, loss = 0.12949886\n",
      "Iteration 196, loss = 0.12921323\n",
      "Iteration 197, loss = 0.12894817\n",
      "Iteration 198, loss = 0.12866196\n",
      "Iteration 199, loss = 0.12838994\n",
      "Iteration 200, loss = 0.12812548\n",
      "Iteration 1, loss = 0.73634459\n",
      "Iteration 2, loss = 0.71577503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.68861469\n",
      "Iteration 4, loss = 0.66000120\n",
      "Iteration 5, loss = 0.63282622\n",
      "Iteration 6, loss = 0.60804841\n",
      "Iteration 7, loss = 0.58586491\n",
      "Iteration 8, loss = 0.56592025\n",
      "Iteration 9, loss = 0.54749551\n",
      "Iteration 10, loss = 0.53045567\n",
      "Iteration 11, loss = 0.51499064\n",
      "Iteration 12, loss = 0.50058793\n",
      "Iteration 13, loss = 0.48698543\n",
      "Iteration 14, loss = 0.47431743\n",
      "Iteration 15, loss = 0.46244071\n",
      "Iteration 16, loss = 0.45116999\n",
      "Iteration 17, loss = 0.44060507\n",
      "Iteration 18, loss = 0.43054535\n",
      "Iteration 19, loss = 0.42107002\n",
      "Iteration 20, loss = 0.41188020\n",
      "Iteration 21, loss = 0.40339365\n",
      "Iteration 22, loss = 0.39518821\n",
      "Iteration 23, loss = 0.38738482\n",
      "Iteration 24, loss = 0.38001080\n",
      "Iteration 25, loss = 0.37283703\n",
      "Iteration 26, loss = 0.36605438\n",
      "Iteration 27, loss = 0.35959100\n",
      "Iteration 28, loss = 0.35333005\n",
      "Iteration 29, loss = 0.34742139\n",
      "Iteration 30, loss = 0.34169497\n",
      "Iteration 31, loss = 0.33620169\n",
      "Iteration 32, loss = 0.33102811\n",
      "Iteration 33, loss = 0.32585319\n",
      "Iteration 34, loss = 0.32105974\n",
      "Iteration 35, loss = 0.31641613\n",
      "Iteration 36, loss = 0.31193790\n",
      "Iteration 37, loss = 0.30756968\n",
      "Iteration 38, loss = 0.30336564\n",
      "Iteration 39, loss = 0.29936925\n",
      "Iteration 40, loss = 0.29545778\n",
      "Iteration 41, loss = 0.29170870\n",
      "Iteration 42, loss = 0.28809482\n",
      "Iteration 43, loss = 0.28460207\n",
      "Iteration 44, loss = 0.28121678\n",
      "Iteration 45, loss = 0.27790423\n",
      "Iteration 46, loss = 0.27477161\n",
      "Iteration 47, loss = 0.27169767\n",
      "Iteration 48, loss = 0.26870289\n",
      "Iteration 49, loss = 0.26588049\n",
      "Iteration 50, loss = 0.26304316\n",
      "Iteration 51, loss = 0.26033825\n",
      "Iteration 52, loss = 0.25772087\n",
      "Iteration 53, loss = 0.25518911\n",
      "Iteration 54, loss = 0.25270211\n",
      "Iteration 55, loss = 0.25028519\n",
      "Iteration 56, loss = 0.24794518\n",
      "Iteration 57, loss = 0.24572231\n",
      "Iteration 58, loss = 0.24348435\n",
      "Iteration 59, loss = 0.24136760\n",
      "Iteration 60, loss = 0.23929434\n",
      "Iteration 61, loss = 0.23722148\n",
      "Iteration 62, loss = 0.23527130\n",
      "Iteration 63, loss = 0.23335522\n",
      "Iteration 64, loss = 0.23147599\n",
      "Iteration 65, loss = 0.22969436\n",
      "Iteration 66, loss = 0.22788973\n",
      "Iteration 67, loss = 0.22614577\n",
      "Iteration 68, loss = 0.22450350\n",
      "Iteration 69, loss = 0.22277171\n",
      "Iteration 70, loss = 0.22118646\n",
      "Iteration 71, loss = 0.21965482\n",
      "Iteration 72, loss = 0.21811187\n",
      "Iteration 73, loss = 0.21659348\n",
      "Iteration 74, loss = 0.21512190\n",
      "Iteration 75, loss = 0.21370449\n",
      "Iteration 76, loss = 0.21233605\n",
      "Iteration 77, loss = 0.21093788\n",
      "Iteration 78, loss = 0.20960867\n",
      "Iteration 79, loss = 0.20830433\n",
      "Iteration 80, loss = 0.20701528\n",
      "Iteration 81, loss = 0.20577636\n",
      "Iteration 82, loss = 0.20453987\n",
      "Iteration 83, loss = 0.20336718\n",
      "Iteration 84, loss = 0.20216157\n",
      "Iteration 85, loss = 0.20104211\n",
      "Iteration 86, loss = 0.19988874\n",
      "Iteration 87, loss = 0.19880236\n",
      "Iteration 88, loss = 0.19770177\n",
      "Iteration 89, loss = 0.19664941\n",
      "Iteration 90, loss = 0.19560201\n",
      "Iteration 91, loss = 0.19458705\n",
      "Iteration 92, loss = 0.19356298\n",
      "Iteration 93, loss = 0.19259992\n",
      "Iteration 94, loss = 0.19163470\n",
      "Iteration 95, loss = 0.19071268\n",
      "Iteration 96, loss = 0.18973470\n",
      "Iteration 97, loss = 0.18882254\n",
      "Iteration 98, loss = 0.18794287\n",
      "Iteration 99, loss = 0.18706111\n",
      "Iteration 100, loss = 0.18617467\n",
      "Iteration 101, loss = 0.18533854\n",
      "Iteration 102, loss = 0.18452145\n",
      "Iteration 103, loss = 0.18368404\n",
      "Iteration 104, loss = 0.18288628\n",
      "Iteration 105, loss = 0.18208799\n",
      "Iteration 106, loss = 0.18130521\n",
      "Iteration 107, loss = 0.18053310\n",
      "Iteration 108, loss = 0.17977859\n",
      "Iteration 109, loss = 0.17903093\n",
      "Iteration 110, loss = 0.17830509\n",
      "Iteration 111, loss = 0.17757349\n",
      "Iteration 112, loss = 0.17688690\n",
      "Iteration 113, loss = 0.17617198\n",
      "Iteration 114, loss = 0.17549576\n",
      "Iteration 115, loss = 0.17479200\n",
      "Iteration 116, loss = 0.17413085\n",
      "Iteration 117, loss = 0.17348069\n",
      "Iteration 118, loss = 0.17283042\n",
      "Iteration 119, loss = 0.17218308\n",
      "Iteration 120, loss = 0.17157083\n",
      "Iteration 121, loss = 0.17095776\n",
      "Iteration 122, loss = 0.17032403\n",
      "Iteration 123, loss = 0.16972145\n",
      "Iteration 124, loss = 0.16911812\n",
      "Iteration 125, loss = 0.16851244\n",
      "Iteration 126, loss = 0.16796256\n",
      "Iteration 127, loss = 0.16737103\n",
      "Iteration 128, loss = 0.16681690\n",
      "Iteration 129, loss = 0.16623773\n",
      "Iteration 130, loss = 0.16569141\n",
      "Iteration 131, loss = 0.16515542\n",
      "Iteration 132, loss = 0.16461463\n",
      "Iteration 133, loss = 0.16407523\n",
      "Iteration 134, loss = 0.16356544\n",
      "Iteration 135, loss = 0.16303915\n",
      "Iteration 136, loss = 0.16253757\n",
      "Iteration 137, loss = 0.16202791\n",
      "Iteration 138, loss = 0.16154914\n",
      "Iteration 139, loss = 0.16103644\n",
      "Iteration 140, loss = 0.16056417\n",
      "Iteration 141, loss = 0.16007254\n",
      "Iteration 142, loss = 0.15960613\n",
      "Iteration 143, loss = 0.15913580\n",
      "Iteration 144, loss = 0.15866714\n",
      "Iteration 145, loss = 0.15821309\n",
      "Iteration 146, loss = 0.15775272\n",
      "Iteration 147, loss = 0.15731017\n",
      "Iteration 148, loss = 0.15685712\n",
      "Iteration 149, loss = 0.15641892\n",
      "Iteration 150, loss = 0.15599999\n",
      "Iteration 151, loss = 0.15555751\n",
      "Iteration 152, loss = 0.15513985\n",
      "Iteration 153, loss = 0.15471373\n",
      "Iteration 154, loss = 0.15430494\n",
      "Iteration 155, loss = 0.15388213\n",
      "Iteration 156, loss = 0.15347444\n",
      "Iteration 157, loss = 0.15306624\n",
      "Iteration 158, loss = 0.15267814\n",
      "Iteration 159, loss = 0.15228216\n",
      "Iteration 160, loss = 0.15189716\n",
      "Iteration 161, loss = 0.15150088\n",
      "Iteration 162, loss = 0.15113211\n",
      "Iteration 163, loss = 0.15073805\n",
      "Iteration 164, loss = 0.15037121\n",
      "Iteration 165, loss = 0.15000309\n",
      "Iteration 166, loss = 0.14963274\n",
      "Iteration 167, loss = 0.14926191\n",
      "Iteration 168, loss = 0.14891864\n",
      "Iteration 169, loss = 0.14856464\n",
      "Iteration 170, loss = 0.14820110\n",
      "Iteration 171, loss = 0.14784674\n",
      "Iteration 172, loss = 0.14750347\n",
      "Iteration 173, loss = 0.14714972\n",
      "Iteration 174, loss = 0.14682457\n",
      "Iteration 175, loss = 0.14647684\n",
      "Iteration 176, loss = 0.14615552\n",
      "Iteration 177, loss = 0.14581573\n",
      "Iteration 178, loss = 0.14548094\n",
      "Iteration 179, loss = 0.14517684\n",
      "Iteration 180, loss = 0.14483455\n",
      "Iteration 181, loss = 0.14452528\n",
      "Iteration 182, loss = 0.14420682\n",
      "Iteration 183, loss = 0.14388797\n",
      "Iteration 184, loss = 0.14357553\n",
      "Iteration 185, loss = 0.14327624\n",
      "Iteration 186, loss = 0.14297003\n",
      "Iteration 187, loss = 0.14265950\n",
      "Iteration 188, loss = 0.14236825\n",
      "Iteration 189, loss = 0.14206560\n",
      "Iteration 190, loss = 0.14176665\n",
      "Iteration 191, loss = 0.14147031\n",
      "Iteration 192, loss = 0.14118911\n",
      "Iteration 193, loss = 0.14089718\n",
      "Iteration 194, loss = 0.14060891\n",
      "Iteration 195, loss = 0.14032245\n",
      "Iteration 196, loss = 0.14003783\n",
      "Iteration 197, loss = 0.13976267\n",
      "Iteration 198, loss = 0.13947623\n",
      "Iteration 199, loss = 0.13919941\n",
      "Iteration 200, loss = 0.13893449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73360751\n",
      "Iteration 2, loss = 0.71330294\n",
      "Iteration 3, loss = 0.68654652\n",
      "Iteration 4, loss = 0.65791153\n",
      "Iteration 5, loss = 0.63113599\n",
      "Iteration 6, loss = 0.60663943\n",
      "Iteration 7, loss = 0.58442438\n",
      "Iteration 8, loss = 0.56456440\n",
      "Iteration 9, loss = 0.54601239\n",
      "Iteration 10, loss = 0.52893754\n",
      "Iteration 11, loss = 0.51340571\n",
      "Iteration 12, loss = 0.49888759\n",
      "Iteration 13, loss = 0.48513693\n",
      "Iteration 14, loss = 0.47237980\n",
      "Iteration 15, loss = 0.46035775\n",
      "Iteration 16, loss = 0.44889778\n",
      "Iteration 17, loss = 0.43817524\n",
      "Iteration 18, loss = 0.42790540\n",
      "Iteration 19, loss = 0.41833633\n",
      "Iteration 20, loss = 0.40896461\n",
      "Iteration 21, loss = 0.40034181\n",
      "Iteration 22, loss = 0.39198454\n",
      "Iteration 23, loss = 0.38398446\n",
      "Iteration 24, loss = 0.37642372\n",
      "Iteration 25, loss = 0.36918304\n",
      "Iteration 26, loss = 0.36224883\n",
      "Iteration 27, loss = 0.35564913\n",
      "Iteration 28, loss = 0.34927273\n",
      "Iteration 29, loss = 0.34321193\n",
      "Iteration 30, loss = 0.33736228\n",
      "Iteration 31, loss = 0.33174256\n",
      "Iteration 32, loss = 0.32646579\n",
      "Iteration 33, loss = 0.32117252\n",
      "Iteration 34, loss = 0.31626938\n",
      "Iteration 35, loss = 0.31151549\n",
      "Iteration 36, loss = 0.30691618\n",
      "Iteration 37, loss = 0.30247568\n",
      "Iteration 38, loss = 0.29818955\n",
      "Iteration 39, loss = 0.29406920\n",
      "Iteration 40, loss = 0.29008745\n",
      "Iteration 41, loss = 0.28626013\n",
      "Iteration 42, loss = 0.28254700\n",
      "Iteration 43, loss = 0.27901697\n",
      "Iteration 44, loss = 0.27552660\n",
      "Iteration 45, loss = 0.27216615\n",
      "Iteration 46, loss = 0.26897173\n",
      "Iteration 47, loss = 0.26582701\n",
      "Iteration 48, loss = 0.26277507\n",
      "Iteration 49, loss = 0.25990161\n",
      "Iteration 50, loss = 0.25705385\n",
      "Iteration 51, loss = 0.25428265\n",
      "Iteration 52, loss = 0.25161066\n",
      "Iteration 53, loss = 0.24904552\n",
      "Iteration 54, loss = 0.24651532\n",
      "Iteration 55, loss = 0.24405550\n",
      "Iteration 56, loss = 0.24168619\n",
      "Iteration 57, loss = 0.23942325\n",
      "Iteration 58, loss = 0.23715448\n",
      "Iteration 59, loss = 0.23499700\n",
      "Iteration 60, loss = 0.23291266\n",
      "Iteration 61, loss = 0.23082341\n",
      "Iteration 62, loss = 0.22884553\n",
      "Iteration 63, loss = 0.22688504\n",
      "Iteration 64, loss = 0.22500441\n",
      "Iteration 65, loss = 0.22318779\n",
      "Iteration 66, loss = 0.22139378\n",
      "Iteration 67, loss = 0.21963135\n",
      "Iteration 68, loss = 0.21795498\n",
      "Iteration 69, loss = 0.21621500\n",
      "Iteration 70, loss = 0.21461289\n",
      "Iteration 71, loss = 0.21307076\n",
      "Iteration 72, loss = 0.21151718\n",
      "Iteration 73, loss = 0.20996981\n",
      "Iteration 74, loss = 0.20849538\n",
      "Iteration 75, loss = 0.20706443\n",
      "Iteration 76, loss = 0.20567512\n",
      "Iteration 77, loss = 0.20428453\n",
      "Iteration 78, loss = 0.20295551\n",
      "Iteration 79, loss = 0.20162423\n",
      "Iteration 80, loss = 0.20033780\n",
      "Iteration 81, loss = 0.19908892\n",
      "Iteration 82, loss = 0.19784446\n",
      "Iteration 83, loss = 0.19667701\n",
      "Iteration 84, loss = 0.19546620\n",
      "Iteration 85, loss = 0.19434223\n",
      "Iteration 86, loss = 0.19317165\n",
      "Iteration 87, loss = 0.19209690\n",
      "Iteration 88, loss = 0.19099394\n",
      "Iteration 89, loss = 0.18993634\n",
      "Iteration 90, loss = 0.18888755\n",
      "Iteration 91, loss = 0.18786966\n",
      "Iteration 92, loss = 0.18686079\n",
      "Iteration 93, loss = 0.18587493\n",
      "Iteration 94, loss = 0.18491067\n",
      "Iteration 95, loss = 0.18397786\n",
      "Iteration 96, loss = 0.18301832\n",
      "Iteration 97, loss = 0.18210190\n",
      "Iteration 98, loss = 0.18121843\n",
      "Iteration 99, loss = 0.18034615\n",
      "Iteration 100, loss = 0.17945792\n",
      "Iteration 101, loss = 0.17861503\n",
      "Iteration 102, loss = 0.17780189\n",
      "Iteration 103, loss = 0.17696555\n",
      "Iteration 104, loss = 0.17616887\n",
      "Iteration 105, loss = 0.17537559\n",
      "Iteration 106, loss = 0.17460089\n",
      "Iteration 107, loss = 0.17382018\n",
      "Iteration 108, loss = 0.17307008\n",
      "Iteration 109, loss = 0.17232448\n",
      "Iteration 110, loss = 0.17159238\n",
      "Iteration 111, loss = 0.17087022\n",
      "Iteration 112, loss = 0.17018729\n",
      "Iteration 113, loss = 0.16947947\n",
      "Iteration 114, loss = 0.16880782\n",
      "Iteration 115, loss = 0.16810129\n",
      "Iteration 116, loss = 0.16744126\n",
      "Iteration 117, loss = 0.16680636\n",
      "Iteration 118, loss = 0.16613928\n",
      "Iteration 119, loss = 0.16549833\n",
      "Iteration 120, loss = 0.16489685\n",
      "Iteration 121, loss = 0.16425850\n",
      "Iteration 122, loss = 0.16365569\n",
      "Iteration 123, loss = 0.16305913\n",
      "Iteration 124, loss = 0.16244143\n",
      "Iteration 125, loss = 0.16184638\n",
      "Iteration 126, loss = 0.16128848\n",
      "Iteration 127, loss = 0.16071831\n",
      "Iteration 128, loss = 0.16014419\n",
      "Iteration 129, loss = 0.15958095\n",
      "Iteration 130, loss = 0.15902992\n",
      "Iteration 131, loss = 0.15849121\n",
      "Iteration 132, loss = 0.15795446\n",
      "Iteration 133, loss = 0.15742110\n",
      "Iteration 134, loss = 0.15691516\n",
      "Iteration 135, loss = 0.15639577\n",
      "Iteration 136, loss = 0.15588219\n",
      "Iteration 137, loss = 0.15537751\n",
      "Iteration 138, loss = 0.15489651\n",
      "Iteration 139, loss = 0.15437879\n",
      "Iteration 140, loss = 0.15390803\n",
      "Iteration 141, loss = 0.15342642\n",
      "Iteration 142, loss = 0.15296055\n",
      "Iteration 143, loss = 0.15249365\n",
      "Iteration 144, loss = 0.15202992\n",
      "Iteration 145, loss = 0.15157106\n",
      "Iteration 146, loss = 0.15111551\n",
      "Iteration 147, loss = 0.15067075\n",
      "Iteration 148, loss = 0.15022602\n",
      "Iteration 149, loss = 0.14978403\n",
      "Iteration 150, loss = 0.14936766\n",
      "Iteration 151, loss = 0.14892485\n",
      "Iteration 152, loss = 0.14852251\n",
      "Iteration 153, loss = 0.14808476\n",
      "Iteration 154, loss = 0.14768372\n",
      "Iteration 155, loss = 0.14726420\n",
      "Iteration 156, loss = 0.14686071\n",
      "Iteration 157, loss = 0.14645719\n",
      "Iteration 158, loss = 0.14605542\n",
      "Iteration 159, loss = 0.14567025\n",
      "Iteration 160, loss = 0.14528365\n",
      "Iteration 161, loss = 0.14489353\n",
      "Iteration 162, loss = 0.14452023\n",
      "Iteration 163, loss = 0.14413504\n",
      "Iteration 164, loss = 0.14376965\n",
      "Iteration 165, loss = 0.14340635\n",
      "Iteration 166, loss = 0.14303014\n",
      "Iteration 167, loss = 0.14265863\n",
      "Iteration 168, loss = 0.14231887\n",
      "Iteration 169, loss = 0.14195847\n",
      "Iteration 170, loss = 0.14160366\n",
      "Iteration 171, loss = 0.14125144\n",
      "Iteration 172, loss = 0.14090514\n",
      "Iteration 173, loss = 0.14055462\n",
      "Iteration 174, loss = 0.14023001\n",
      "Iteration 175, loss = 0.13987597\n",
      "Iteration 176, loss = 0.13956075\n",
      "Iteration 177, loss = 0.13921957\n",
      "Iteration 178, loss = 0.13888567\n",
      "Iteration 179, loss = 0.13857420\n",
      "Iteration 180, loss = 0.13824381\n",
      "Iteration 181, loss = 0.13792778\n",
      "Iteration 182, loss = 0.13760627\n",
      "Iteration 183, loss = 0.13730545\n",
      "Iteration 184, loss = 0.13698388\n",
      "Iteration 185, loss = 0.13668012\n",
      "Iteration 186, loss = 0.13637057\n",
      "Iteration 187, loss = 0.13607000\n",
      "Iteration 188, loss = 0.13578052\n",
      "Iteration 189, loss = 0.13547744\n",
      "Iteration 190, loss = 0.13517890\n",
      "Iteration 191, loss = 0.13488290\n",
      "Iteration 192, loss = 0.13460321\n",
      "Iteration 193, loss = 0.13431299\n",
      "Iteration 194, loss = 0.13402633\n",
      "Iteration 195, loss = 0.13373773\n",
      "Iteration 196, loss = 0.13345909\n",
      "Iteration 197, loss = 0.13318570\n",
      "Iteration 198, loss = 0.13289984\n",
      "Iteration 199, loss = 0.13262433\n",
      "Iteration 200, loss = 0.13236781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73243755\n",
      "Iteration 2, loss = 0.71193017\n",
      "Iteration 3, loss = 0.68483057\n",
      "Iteration 4, loss = 0.65598560\n",
      "Iteration 5, loss = 0.62889699\n",
      "Iteration 6, loss = 0.60428298\n",
      "Iteration 7, loss = 0.58201511\n",
      "Iteration 8, loss = 0.56201106\n",
      "Iteration 9, loss = 0.54345685\n",
      "Iteration 10, loss = 0.52642625\n",
      "Iteration 11, loss = 0.51090192\n",
      "Iteration 12, loss = 0.49638712\n",
      "Iteration 13, loss = 0.48277064\n",
      "Iteration 14, loss = 0.47016515\n",
      "Iteration 15, loss = 0.45820369\n",
      "Iteration 16, loss = 0.44688486\n",
      "Iteration 17, loss = 0.43626473\n",
      "Iteration 18, loss = 0.42624406\n",
      "Iteration 19, loss = 0.41678819\n",
      "Iteration 20, loss = 0.40762954\n",
      "Iteration 21, loss = 0.39910185\n",
      "Iteration 22, loss = 0.39097145\n",
      "Iteration 23, loss = 0.38315203\n",
      "Iteration 24, loss = 0.37576692\n",
      "Iteration 25, loss = 0.36866243\n",
      "Iteration 26, loss = 0.36192917\n",
      "Iteration 27, loss = 0.35551441\n",
      "Iteration 28, loss = 0.34929280\n",
      "Iteration 29, loss = 0.34333507\n",
      "Iteration 30, loss = 0.33771537\n",
      "Iteration 31, loss = 0.33223472\n",
      "Iteration 32, loss = 0.32708059\n",
      "Iteration 33, loss = 0.32194842\n",
      "Iteration 34, loss = 0.31719863\n",
      "Iteration 35, loss = 0.31257974\n",
      "Iteration 36, loss = 0.30811871\n",
      "Iteration 37, loss = 0.30378972\n",
      "Iteration 38, loss = 0.29963966\n",
      "Iteration 39, loss = 0.29566339\n",
      "Iteration 40, loss = 0.29176379\n",
      "Iteration 41, loss = 0.28807118\n",
      "Iteration 42, loss = 0.28447723\n",
      "Iteration 43, loss = 0.28102968\n",
      "Iteration 44, loss = 0.27763889\n",
      "Iteration 45, loss = 0.27440182\n",
      "Iteration 46, loss = 0.27125276\n",
      "Iteration 47, loss = 0.26822821\n",
      "Iteration 48, loss = 0.26526307\n",
      "Iteration 49, loss = 0.26243647\n",
      "Iteration 50, loss = 0.25965229\n",
      "Iteration 51, loss = 0.25697736\n",
      "Iteration 52, loss = 0.25435981\n",
      "Iteration 53, loss = 0.25186697\n",
      "Iteration 54, loss = 0.24940520\n",
      "Iteration 55, loss = 0.24698192\n",
      "Iteration 56, loss = 0.24468224\n",
      "Iteration 57, loss = 0.24245829\n",
      "Iteration 58, loss = 0.24024494\n",
      "Iteration 59, loss = 0.23814331\n",
      "Iteration 60, loss = 0.23608998\n",
      "Iteration 61, loss = 0.23404413\n",
      "Iteration 62, loss = 0.23211917\n",
      "Iteration 63, loss = 0.23018726\n",
      "Iteration 64, loss = 0.22833705\n",
      "Iteration 65, loss = 0.22656231\n",
      "Iteration 66, loss = 0.22476360\n",
      "Iteration 67, loss = 0.22306197\n",
      "Iteration 68, loss = 0.22138160\n",
      "Iteration 69, loss = 0.21970101\n",
      "Iteration 70, loss = 0.21810878\n",
      "Iteration 71, loss = 0.21656440\n",
      "Iteration 72, loss = 0.21503424\n",
      "Iteration 73, loss = 0.21351286\n",
      "Iteration 74, loss = 0.21205097\n",
      "Iteration 75, loss = 0.21060859\n",
      "Iteration 76, loss = 0.20925457\n",
      "Iteration 77, loss = 0.20786297\n",
      "Iteration 78, loss = 0.20655127\n",
      "Iteration 79, loss = 0.20521023\n",
      "Iteration 80, loss = 0.20393101\n",
      "Iteration 81, loss = 0.20269283\n",
      "Iteration 82, loss = 0.20145646\n",
      "Iteration 83, loss = 0.20026587\n",
      "Iteration 84, loss = 0.19906219\n",
      "Iteration 85, loss = 0.19794785\n",
      "Iteration 86, loss = 0.19678178\n",
      "Iteration 87, loss = 0.19569110\n",
      "Iteration 88, loss = 0.19459176\n",
      "Iteration 89, loss = 0.19351680\n",
      "Iteration 90, loss = 0.19247258\n",
      "Iteration 91, loss = 0.19144157\n",
      "Iteration 92, loss = 0.19044224\n",
      "Iteration 93, loss = 0.18944186\n",
      "Iteration 94, loss = 0.18846961\n",
      "Iteration 95, loss = 0.18751321\n",
      "Iteration 96, loss = 0.18655818\n",
      "Iteration 97, loss = 0.18565137\n",
      "Iteration 98, loss = 0.18474433\n",
      "Iteration 99, loss = 0.18386591\n",
      "Iteration 100, loss = 0.18295616\n",
      "Iteration 101, loss = 0.18211263\n",
      "Iteration 102, loss = 0.18127580\n",
      "Iteration 103, loss = 0.18042625\n",
      "Iteration 104, loss = 0.17961473\n",
      "Iteration 105, loss = 0.17882653\n",
      "Iteration 106, loss = 0.17800919\n",
      "Iteration 107, loss = 0.17722867\n",
      "Iteration 108, loss = 0.17646468\n",
      "Iteration 109, loss = 0.17569859\n",
      "Iteration 110, loss = 0.17495713\n",
      "Iteration 111, loss = 0.17421047\n",
      "Iteration 112, loss = 0.17351012\n",
      "Iteration 113, loss = 0.17279949\n",
      "Iteration 114, loss = 0.17210511\n",
      "Iteration 115, loss = 0.17137766\n",
      "Iteration 116, loss = 0.17069950\n",
      "Iteration 117, loss = 0.17005178\n",
      "Iteration 118, loss = 0.16936804\n",
      "Iteration 119, loss = 0.16870154\n",
      "Iteration 120, loss = 0.16808843\n",
      "Iteration 121, loss = 0.16742661\n",
      "Iteration 122, loss = 0.16680304\n",
      "Iteration 123, loss = 0.16619614\n",
      "Iteration 124, loss = 0.16556889\n",
      "Iteration 125, loss = 0.16496498\n",
      "Iteration 126, loss = 0.16437986\n",
      "Iteration 127, loss = 0.16379787\n",
      "Iteration 128, loss = 0.16320085\n",
      "Iteration 129, loss = 0.16263530\n",
      "Iteration 130, loss = 0.16204862\n",
      "Iteration 131, loss = 0.16150598\n",
      "Iteration 132, loss = 0.16095546\n",
      "Iteration 133, loss = 0.16040101\n",
      "Iteration 134, loss = 0.15988074\n",
      "Iteration 135, loss = 0.15934439\n",
      "Iteration 136, loss = 0.15881090\n",
      "Iteration 137, loss = 0.15829945\n",
      "Iteration 138, loss = 0.15779371\n",
      "Iteration 139, loss = 0.15725754\n",
      "Iteration 140, loss = 0.15677582\n",
      "Iteration 141, loss = 0.15627915\n",
      "Iteration 142, loss = 0.15578560\n",
      "Iteration 143, loss = 0.15532109\n",
      "Iteration 144, loss = 0.15482769\n",
      "Iteration 145, loss = 0.15435512\n",
      "Iteration 146, loss = 0.15389121\n",
      "Iteration 147, loss = 0.15342882\n",
      "Iteration 148, loss = 0.15297090\n",
      "Iteration 149, loss = 0.15250722\n",
      "Iteration 150, loss = 0.15208183\n",
      "Iteration 151, loss = 0.15161887\n",
      "Iteration 152, loss = 0.15119876\n",
      "Iteration 153, loss = 0.15074784\n",
      "Iteration 154, loss = 0.15032818\n",
      "Iteration 155, loss = 0.14989366\n",
      "Iteration 156, loss = 0.14947857\n",
      "Iteration 157, loss = 0.14905646\n",
      "Iteration 158, loss = 0.14864355\n",
      "Iteration 159, loss = 0.14824780\n",
      "Iteration 160, loss = 0.14783700\n",
      "Iteration 161, loss = 0.14744224\n",
      "Iteration 162, loss = 0.14704687\n",
      "Iteration 163, loss = 0.14664685\n",
      "Iteration 164, loss = 0.14627403\n",
      "Iteration 165, loss = 0.14588644\n",
      "Iteration 166, loss = 0.14550254\n",
      "Iteration 167, loss = 0.14512769\n",
      "Iteration 168, loss = 0.14477130\n",
      "Iteration 169, loss = 0.14439483\n",
      "Iteration 170, loss = 0.14403303\n",
      "Iteration 171, loss = 0.14366571\n",
      "Iteration 172, loss = 0.14330037\n",
      "Iteration 173, loss = 0.14295021\n",
      "Iteration 174, loss = 0.14261301\n",
      "Iteration 175, loss = 0.14224237\n",
      "Iteration 176, loss = 0.14191451\n",
      "Iteration 177, loss = 0.14155996\n",
      "Iteration 178, loss = 0.14121594\n",
      "Iteration 179, loss = 0.14089019\n",
      "Iteration 180, loss = 0.14055151\n",
      "Iteration 181, loss = 0.14022886\n",
      "Iteration 182, loss = 0.13989394\n",
      "Iteration 183, loss = 0.13958639\n",
      "Iteration 184, loss = 0.13925018\n",
      "Iteration 185, loss = 0.13893815\n",
      "Iteration 186, loss = 0.13861960\n",
      "Iteration 187, loss = 0.13830310\n",
      "Iteration 188, loss = 0.13801004\n",
      "Iteration 189, loss = 0.13768742\n",
      "Iteration 190, loss = 0.13738285\n",
      "Iteration 191, loss = 0.13708065\n",
      "Iteration 192, loss = 0.13678610\n",
      "Iteration 193, loss = 0.13648786\n",
      "Iteration 194, loss = 0.13619264\n",
      "Iteration 195, loss = 0.13589914\n",
      "Iteration 196, loss = 0.13560927\n",
      "Iteration 197, loss = 0.13532565\n",
      "Iteration 198, loss = 0.13503283\n",
      "Iteration 199, loss = 0.13474649\n",
      "Iteration 200, loss = 0.13448487\n",
      "Iteration 1, loss = 0.73244817\n",
      "Iteration 2, loss = 0.71285032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.68670670\n",
      "Iteration 4, loss = 0.65899984\n",
      "Iteration 5, loss = 0.63262389\n",
      "Iteration 6, loss = 0.60864263\n",
      "Iteration 7, loss = 0.58669578\n",
      "Iteration 8, loss = 0.56699374\n",
      "Iteration 9, loss = 0.54873012\n",
      "Iteration 10, loss = 0.53189130\n",
      "Iteration 11, loss = 0.51641341\n",
      "Iteration 12, loss = 0.50196726\n",
      "Iteration 13, loss = 0.48838159\n",
      "Iteration 14, loss = 0.47575930\n",
      "Iteration 15, loss = 0.46375151\n",
      "Iteration 16, loss = 0.45240619\n",
      "Iteration 17, loss = 0.44171655\n",
      "Iteration 18, loss = 0.43162676\n",
      "Iteration 19, loss = 0.42207820\n",
      "Iteration 20, loss = 0.41288266\n",
      "Iteration 21, loss = 0.40423686\n",
      "Iteration 22, loss = 0.39598720\n",
      "Iteration 23, loss = 0.38809252\n",
      "Iteration 24, loss = 0.38057280\n",
      "Iteration 25, loss = 0.37335890\n",
      "Iteration 26, loss = 0.36651486\n",
      "Iteration 27, loss = 0.36001031\n",
      "Iteration 28, loss = 0.35364120\n",
      "Iteration 29, loss = 0.34755217\n",
      "Iteration 30, loss = 0.34184060\n",
      "Iteration 31, loss = 0.33621400\n",
      "Iteration 32, loss = 0.33096435\n",
      "Iteration 33, loss = 0.32571489\n",
      "Iteration 34, loss = 0.32083421\n",
      "Iteration 35, loss = 0.31612993\n",
      "Iteration 36, loss = 0.31154400\n",
      "Iteration 37, loss = 0.30708724\n",
      "Iteration 38, loss = 0.30283802\n",
      "Iteration 39, loss = 0.29877750\n",
      "Iteration 40, loss = 0.29475173\n",
      "Iteration 41, loss = 0.29096208\n",
      "Iteration 42, loss = 0.28729718\n",
      "Iteration 43, loss = 0.28374597\n",
      "Iteration 44, loss = 0.28025679\n",
      "Iteration 45, loss = 0.27693107\n",
      "Iteration 46, loss = 0.27369724\n",
      "Iteration 47, loss = 0.27058714\n",
      "Iteration 48, loss = 0.26753831\n",
      "Iteration 49, loss = 0.26463265\n",
      "Iteration 50, loss = 0.26176329\n",
      "Iteration 51, loss = 0.25902783\n",
      "Iteration 52, loss = 0.25631517\n",
      "Iteration 53, loss = 0.25376950\n",
      "Iteration 54, loss = 0.25123836\n",
      "Iteration 55, loss = 0.24872989\n",
      "Iteration 56, loss = 0.24637419\n",
      "Iteration 57, loss = 0.24408675\n",
      "Iteration 58, loss = 0.24179735\n",
      "Iteration 59, loss = 0.23965200\n",
      "Iteration 60, loss = 0.23755479\n",
      "Iteration 61, loss = 0.23543284\n",
      "Iteration 62, loss = 0.23346620\n",
      "Iteration 63, loss = 0.23146798\n",
      "Iteration 64, loss = 0.22959247\n",
      "Iteration 65, loss = 0.22772963\n",
      "Iteration 66, loss = 0.22590889\n",
      "Iteration 67, loss = 0.22415922\n",
      "Iteration 68, loss = 0.22241874\n",
      "Iteration 69, loss = 0.22073371\n",
      "Iteration 70, loss = 0.21908893\n",
      "Iteration 71, loss = 0.21749623\n",
      "Iteration 72, loss = 0.21593098\n",
      "Iteration 73, loss = 0.21438987\n",
      "Iteration 74, loss = 0.21289380\n",
      "Iteration 75, loss = 0.21141991\n",
      "Iteration 76, loss = 0.21002705\n",
      "Iteration 77, loss = 0.20861402\n",
      "Iteration 78, loss = 0.20728290\n",
      "Iteration 79, loss = 0.20590544\n",
      "Iteration 80, loss = 0.20461194\n",
      "Iteration 81, loss = 0.20334886\n",
      "Iteration 82, loss = 0.20208402\n",
      "Iteration 83, loss = 0.20087559\n",
      "Iteration 84, loss = 0.19965256\n",
      "Iteration 85, loss = 0.19850934\n",
      "Iteration 86, loss = 0.19734614\n",
      "Iteration 87, loss = 0.19621447\n",
      "Iteration 88, loss = 0.19511657\n",
      "Iteration 89, loss = 0.19401335\n",
      "Iteration 90, loss = 0.19294709\n",
      "Iteration 91, loss = 0.19192165\n",
      "Iteration 92, loss = 0.19090593\n",
      "Iteration 93, loss = 0.18988879\n",
      "Iteration 94, loss = 0.18890808\n",
      "Iteration 95, loss = 0.18793664\n",
      "Iteration 96, loss = 0.18698037\n",
      "Iteration 97, loss = 0.18606552\n",
      "Iteration 98, loss = 0.18515813\n",
      "Iteration 99, loss = 0.18426044\n",
      "Iteration 100, loss = 0.18336138\n",
      "Iteration 101, loss = 0.18250317\n",
      "Iteration 102, loss = 0.18166057\n",
      "Iteration 103, loss = 0.18081167\n",
      "Iteration 104, loss = 0.17998653\n",
      "Iteration 105, loss = 0.17919735\n",
      "Iteration 106, loss = 0.17837251\n",
      "Iteration 107, loss = 0.17759315\n",
      "Iteration 108, loss = 0.17683809\n",
      "Iteration 109, loss = 0.17606037\n",
      "Iteration 110, loss = 0.17531384\n",
      "Iteration 111, loss = 0.17456644\n",
      "Iteration 112, loss = 0.17387151\n",
      "Iteration 113, loss = 0.17315738\n",
      "Iteration 114, loss = 0.17246091\n",
      "Iteration 115, loss = 0.17174178\n",
      "Iteration 116, loss = 0.17107127\n",
      "Iteration 117, loss = 0.17043044\n",
      "Iteration 118, loss = 0.16975080\n",
      "Iteration 119, loss = 0.16908400\n",
      "Iteration 120, loss = 0.16846893\n",
      "Iteration 121, loss = 0.16781864\n",
      "Iteration 122, loss = 0.16719905\n",
      "Iteration 123, loss = 0.16659746\n",
      "Iteration 124, loss = 0.16597068\n",
      "Iteration 125, loss = 0.16537553\n",
      "Iteration 126, loss = 0.16479792\n",
      "Iteration 127, loss = 0.16422522\n",
      "Iteration 128, loss = 0.16363688\n",
      "Iteration 129, loss = 0.16307865\n",
      "Iteration 130, loss = 0.16249688\n",
      "Iteration 131, loss = 0.16196256\n",
      "Iteration 132, loss = 0.16141637\n",
      "Iteration 133, loss = 0.16087436\n",
      "Iteration 134, loss = 0.16036305\n",
      "Iteration 135, loss = 0.15983066\n",
      "Iteration 136, loss = 0.15930046\n",
      "Iteration 137, loss = 0.15880894\n",
      "Iteration 138, loss = 0.15829938\n",
      "Iteration 139, loss = 0.15778638\n",
      "Iteration 140, loss = 0.15731096\n",
      "Iteration 141, loss = 0.15682552\n",
      "Iteration 142, loss = 0.15633926\n",
      "Iteration 143, loss = 0.15587510\n",
      "Iteration 144, loss = 0.15539579\n",
      "Iteration 145, loss = 0.15493181\n",
      "Iteration 146, loss = 0.15447277\n",
      "Iteration 147, loss = 0.15402874\n",
      "Iteration 148, loss = 0.15357559\n",
      "Iteration 149, loss = 0.15311973\n",
      "Iteration 150, loss = 0.15270585\n",
      "Iteration 151, loss = 0.15225686\n",
      "Iteration 152, loss = 0.15184769\n",
      "Iteration 153, loss = 0.15140074\n",
      "Iteration 154, loss = 0.15099670\n",
      "Iteration 155, loss = 0.15057406\n",
      "Iteration 156, loss = 0.15016674\n",
      "Iteration 157, loss = 0.14975406\n",
      "Iteration 158, loss = 0.14934718\n",
      "Iteration 159, loss = 0.14896708\n",
      "Iteration 160, loss = 0.14856507\n",
      "Iteration 161, loss = 0.14818916\n",
      "Iteration 162, loss = 0.14780100\n",
      "Iteration 163, loss = 0.14740360\n",
      "Iteration 164, loss = 0.14704299\n",
      "Iteration 165, loss = 0.14666731\n",
      "Iteration 166, loss = 0.14629408\n",
      "Iteration 167, loss = 0.14593059\n",
      "Iteration 168, loss = 0.14557672\n",
      "Iteration 169, loss = 0.14521838\n",
      "Iteration 170, loss = 0.14485940\n",
      "Iteration 171, loss = 0.14450468\n",
      "Iteration 172, loss = 0.14415279\n",
      "Iteration 173, loss = 0.14381101\n",
      "Iteration 174, loss = 0.14347935\n",
      "Iteration 175, loss = 0.14312488\n",
      "Iteration 176, loss = 0.14280750\n",
      "Iteration 177, loss = 0.14246311\n",
      "Iteration 178, loss = 0.14213574\n",
      "Iteration 179, loss = 0.14181407\n",
      "Iteration 180, loss = 0.14148798\n",
      "Iteration 181, loss = 0.14117651\n",
      "Iteration 182, loss = 0.14085584\n",
      "Iteration 183, loss = 0.14054936\n",
      "Iteration 184, loss = 0.14023109\n",
      "Iteration 185, loss = 0.13992935\n",
      "Iteration 186, loss = 0.13961504\n",
      "Iteration 187, loss = 0.13931004\n",
      "Iteration 188, loss = 0.13901986\n",
      "Iteration 189, loss = 0.13871565\n",
      "Iteration 190, loss = 0.13841541\n",
      "Iteration 191, loss = 0.13812028\n",
      "Iteration 192, loss = 0.13783906\n",
      "Iteration 193, loss = 0.13754545\n",
      "Iteration 194, loss = 0.13725770\n",
      "Iteration 195, loss = 0.13697616\n",
      "Iteration 196, loss = 0.13669291\n",
      "Iteration 197, loss = 0.13641890\n",
      "Iteration 198, loss = 0.13613150\n",
      "Iteration 199, loss = 0.13585601\n",
      "Iteration 200, loss = 0.13560155\n",
      "Iteration 1, loss = 0.73363158\n",
      "Iteration 2, loss = 0.71361449\n",
      "Iteration 3, loss = 0.68719125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.65920361\n",
      "Iteration 5, loss = 0.63239662\n",
      "Iteration 6, loss = 0.60833562\n",
      "Iteration 7, loss = 0.58658102\n",
      "Iteration 8, loss = 0.56681551\n",
      "Iteration 9, loss = 0.54883257\n",
      "Iteration 10, loss = 0.53216241\n",
      "Iteration 11, loss = 0.51689719\n",
      "Iteration 12, loss = 0.50276562\n",
      "Iteration 13, loss = 0.48941104\n",
      "Iteration 14, loss = 0.47705297\n",
      "Iteration 15, loss = 0.46532412\n",
      "Iteration 16, loss = 0.45420658\n",
      "Iteration 17, loss = 0.44374413\n",
      "Iteration 18, loss = 0.43392368\n",
      "Iteration 19, loss = 0.42457455\n",
      "Iteration 20, loss = 0.41559490\n",
      "Iteration 21, loss = 0.40712599\n",
      "Iteration 22, loss = 0.39909732\n",
      "Iteration 23, loss = 0.39136526\n",
      "Iteration 24, loss = 0.38405110\n",
      "Iteration 25, loss = 0.37701448\n",
      "Iteration 26, loss = 0.37028155\n",
      "Iteration 27, loss = 0.36395494\n",
      "Iteration 28, loss = 0.35772578\n",
      "Iteration 29, loss = 0.35177556\n",
      "Iteration 30, loss = 0.34619024\n",
      "Iteration 31, loss = 0.34068300\n",
      "Iteration 32, loss = 0.33556246\n",
      "Iteration 33, loss = 0.33043515\n",
      "Iteration 34, loss = 0.32564232\n",
      "Iteration 35, loss = 0.32103521\n",
      "Iteration 36, loss = 0.31655039\n",
      "Iteration 37, loss = 0.31218019\n",
      "Iteration 38, loss = 0.30803642\n",
      "Iteration 39, loss = 0.30407345\n",
      "Iteration 40, loss = 0.30012000\n",
      "Iteration 41, loss = 0.29639569\n",
      "Iteration 42, loss = 0.29278976\n",
      "Iteration 43, loss = 0.28932536\n",
      "Iteration 44, loss = 0.28588753\n",
      "Iteration 45, loss = 0.28263775\n",
      "Iteration 46, loss = 0.27946727\n",
      "Iteration 47, loss = 0.27639845\n",
      "Iteration 48, loss = 0.27340328\n",
      "Iteration 49, loss = 0.27053202\n",
      "Iteration 50, loss = 0.26772407\n",
      "Iteration 51, loss = 0.26504173\n",
      "Iteration 52, loss = 0.26236951\n",
      "Iteration 53, loss = 0.25985705\n",
      "Iteration 54, loss = 0.25738480\n",
      "Iteration 55, loss = 0.25491306\n",
      "Iteration 56, loss = 0.25257255\n",
      "Iteration 57, loss = 0.25032394\n",
      "Iteration 58, loss = 0.24806896\n",
      "Iteration 59, loss = 0.24594960\n",
      "Iteration 60, loss = 0.24386004\n",
      "Iteration 61, loss = 0.24178072\n",
      "Iteration 62, loss = 0.23983034\n",
      "Iteration 63, loss = 0.23785292\n",
      "Iteration 64, loss = 0.23599505\n",
      "Iteration 65, loss = 0.23414317\n",
      "Iteration 66, loss = 0.23233688\n",
      "Iteration 67, loss = 0.23060622\n",
      "Iteration 68, loss = 0.22887363\n",
      "Iteration 69, loss = 0.22718127\n",
      "Iteration 70, loss = 0.22555618\n",
      "Iteration 71, loss = 0.22398041\n",
      "Iteration 72, loss = 0.22242244\n",
      "Iteration 73, loss = 0.22087200\n",
      "Iteration 74, loss = 0.21938331\n",
      "Iteration 75, loss = 0.21790771\n",
      "Iteration 76, loss = 0.21652250\n",
      "Iteration 77, loss = 0.21510494\n",
      "Iteration 78, loss = 0.21377149\n",
      "Iteration 79, loss = 0.21241023\n",
      "Iteration 80, loss = 0.21111000\n",
      "Iteration 81, loss = 0.20983883\n",
      "Iteration 82, loss = 0.20857995\n",
      "Iteration 83, loss = 0.20735802\n",
      "Iteration 84, loss = 0.20613637\n",
      "Iteration 85, loss = 0.20499146\n",
      "Iteration 86, loss = 0.20382895\n",
      "Iteration 87, loss = 0.20270343\n",
      "Iteration 88, loss = 0.20158359\n",
      "Iteration 89, loss = 0.20048102\n",
      "Iteration 90, loss = 0.19940590\n",
      "Iteration 91, loss = 0.19837215\n",
      "Iteration 92, loss = 0.19735035\n",
      "Iteration 93, loss = 0.19632118\n",
      "Iteration 94, loss = 0.19533269\n",
      "Iteration 95, loss = 0.19435595\n",
      "Iteration 96, loss = 0.19339965\n",
      "Iteration 97, loss = 0.19247073\n",
      "Iteration 98, loss = 0.19154827\n",
      "Iteration 99, loss = 0.19064364\n",
      "Iteration 100, loss = 0.18973984\n",
      "Iteration 101, loss = 0.18887063\n",
      "Iteration 102, loss = 0.18802099\n",
      "Iteration 103, loss = 0.18715356\n",
      "Iteration 104, loss = 0.18633133\n",
      "Iteration 105, loss = 0.18552422\n",
      "Iteration 106, loss = 0.18469081\n",
      "Iteration 107, loss = 0.18388982\n",
      "Iteration 108, loss = 0.18312367\n",
      "Iteration 109, loss = 0.18233956\n",
      "Iteration 110, loss = 0.18158215\n",
      "Iteration 111, loss = 0.18081213\n",
      "Iteration 112, loss = 0.18011902\n",
      "Iteration 113, loss = 0.17937702\n",
      "Iteration 114, loss = 0.17867265\n",
      "Iteration 115, loss = 0.17794004\n",
      "Iteration 116, loss = 0.17725516\n",
      "Iteration 117, loss = 0.17660032\n",
      "Iteration 118, loss = 0.17589982\n",
      "Iteration 119, loss = 0.17522608\n",
      "Iteration 120, loss = 0.17458536\n",
      "Iteration 121, loss = 0.17392136\n",
      "Iteration 122, loss = 0.17328871\n",
      "Iteration 123, loss = 0.17267585\n",
      "Iteration 124, loss = 0.17202799\n",
      "Iteration 125, loss = 0.17142578\n",
      "Iteration 126, loss = 0.17082973\n",
      "Iteration 127, loss = 0.17024359\n",
      "Iteration 128, loss = 0.16963348\n",
      "Iteration 129, loss = 0.16906564\n",
      "Iteration 130, loss = 0.16846958\n",
      "Iteration 131, loss = 0.16791813\n",
      "Iteration 132, loss = 0.16734782\n",
      "Iteration 133, loss = 0.16680905\n",
      "Iteration 134, loss = 0.16627381\n",
      "Iteration 135, loss = 0.16572471\n",
      "Iteration 136, loss = 0.16517881\n",
      "Iteration 137, loss = 0.16466655\n",
      "Iteration 138, loss = 0.16415037\n",
      "Iteration 139, loss = 0.16361891\n",
      "Iteration 140, loss = 0.16312640\n",
      "Iteration 141, loss = 0.16262898\n",
      "Iteration 142, loss = 0.16212692\n",
      "Iteration 143, loss = 0.16164754\n",
      "Iteration 144, loss = 0.16115486\n",
      "Iteration 145, loss = 0.16067929\n",
      "Iteration 146, loss = 0.16019982\n",
      "Iteration 147, loss = 0.15974356\n",
      "Iteration 148, loss = 0.15927721\n",
      "Iteration 149, loss = 0.15880721\n",
      "Iteration 150, loss = 0.15838250\n",
      "Iteration 151, loss = 0.15791230\n",
      "Iteration 152, loss = 0.15749425\n",
      "Iteration 153, loss = 0.15702761\n",
      "Iteration 154, loss = 0.15661374\n",
      "Iteration 155, loss = 0.15617066\n",
      "Iteration 156, loss = 0.15575489\n",
      "Iteration 157, loss = 0.15533151\n",
      "Iteration 158, loss = 0.15490314\n",
      "Iteration 159, loss = 0.15450707\n",
      "Iteration 160, loss = 0.15408974\n",
      "Iteration 161, loss = 0.15370604\n",
      "Iteration 162, loss = 0.15330087\n",
      "Iteration 163, loss = 0.15289061\n",
      "Iteration 164, loss = 0.15251815\n",
      "Iteration 165, loss = 0.15212941\n",
      "Iteration 166, loss = 0.15174086\n",
      "Iteration 167, loss = 0.15136270\n",
      "Iteration 168, loss = 0.15099492\n",
      "Iteration 169, loss = 0.15062611\n",
      "Iteration 170, loss = 0.15025239\n",
      "Iteration 171, loss = 0.14988658\n",
      "Iteration 172, loss = 0.14951891\n",
      "Iteration 173, loss = 0.14916539\n",
      "Iteration 174, loss = 0.14882328\n",
      "Iteration 175, loss = 0.14845482\n",
      "Iteration 176, loss = 0.14812205\n",
      "Iteration 177, loss = 0.14777080\n",
      "Iteration 178, loss = 0.14742592\n",
      "Iteration 179, loss = 0.14709218\n",
      "Iteration 180, loss = 0.14675290\n",
      "Iteration 181, loss = 0.14642800\n",
      "Iteration 182, loss = 0.14609677\n",
      "Iteration 183, loss = 0.14577161\n",
      "Iteration 184, loss = 0.14544385\n",
      "Iteration 185, loss = 0.14513323\n",
      "Iteration 186, loss = 0.14480191\n",
      "Iteration 187, loss = 0.14449036\n",
      "Iteration 188, loss = 0.14419271\n",
      "Iteration 189, loss = 0.14387383\n",
      "Iteration 190, loss = 0.14356162\n",
      "Iteration 191, loss = 0.14325642\n",
      "Iteration 192, loss = 0.14296602\n",
      "Iteration 193, loss = 0.14266173\n",
      "Iteration 194, loss = 0.14236306\n",
      "Iteration 195, loss = 0.14207093\n",
      "Iteration 196, loss = 0.14178042\n",
      "Iteration 197, loss = 0.14150073\n",
      "Iteration 198, loss = 0.14120301\n",
      "Iteration 199, loss = 0.14091406\n",
      "Iteration 200, loss = 0.14065117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.95384615, 0.96153846, 0.91538462, 0.953125  , 0.9296875 ,\n",
       "       0.9765625 , 0.9296875 , 0.9609375 , 0.96875   , 0.984375  ])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(mlp, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "Training:\n",
      "AUC:0.989\n",
      "accuracy:0.960\n",
      "recall:0.958\n",
      "precision:0.963\n",
      "specificity:0.963\n",
      " \n"
     ]
    }
   ],
   "source": [
    "y_train_preds = mlp.predict_proba(X_train)[:,1]\n",
    "\n",
    "print('MLP')\n",
    "print('Training:')\n",
    "mlp_train_auc, mlp_train_accuracy, mlp_train_recall, mlp_train_precision, mlp_train_specificity =print_report(y_train,y_train_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf= sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = mlp.predict_proba(X_test_tf)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:\n",
      "AUC:0.991\n",
      "accuracy:0.070\n",
      "recall:1.000\n",
      "precision:0.070\n",
      "specificity:0.000\n",
      " \n"
     ]
    }
   ],
   "source": [
    "thresh=0.\n",
    "print('Test:')\n",
    "test_auc, test_accuracy, test_recall, test_precision, test_specificity = print_report(y_test,y_test_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
